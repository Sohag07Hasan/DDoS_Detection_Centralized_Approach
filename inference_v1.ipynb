{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424c0338-ba52-45c7-b636-7c05ff16352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from dataloader import get_evaluation_datasets_by_client  # Assuming this function gets local client datasets\n",
    "from model import Net\n",
    "from collections import OrderedDict\n",
    "from config import NUM_CLASSES, MODEL_PATH, BATCH_SIZE\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import to_tensor\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef994e1c-6907-4e54-8e2a-e623db313884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ad0a5-4b91-474f-97ef-a0f36ceacc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de8f59b-1122-4aab-aec3-84239c5baf44",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5a2eaf-1391-4c5e-a4cb-ebc40517757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the global model from the saved path\n",
    "def load_model(input_size, num_classes=NUM_CLASSES, model_path=MODEL_PATH):\n",
    "    model = Net(input_size=input_size, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e4f19d-02cd-42b4-a9ca-97dadfb76ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a client's dataset\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Get the total number of samples from the DataLoader\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    # Start the timer before the loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # End the timer after the loop\n",
    "    end_time = time.time()    \n",
    "    # Calculate the total inference time\n",
    "    total_inference_time = end_time - start_time    \n",
    "    # Calculate average inference time per sample\n",
    "    #inference_time_per_sample =  total_inference_time * 1000\n",
    "    inference_time_per_sample =  total_inference_time * 1000000 / total_samples\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), f'{inference_time_per_sample:.4f} us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106d4a49-75bd-4cc1-b995-0fdc5e26b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional: Set a title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59336a77-e75c-4299-9b8b-d9be081b3510",
   "metadata": {},
   "source": [
    "## 2. Performance/History of Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bf58d5-b6f7-4295-8907-44d74e8bdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sources = {\n",
    "    'components': range(4, 16),\n",
    "    'folds': [1, 2, 3, 4, 5],\n",
    "    'marker': ['o', '-', '^' 'x', '-o-'],\n",
    "    'clients': [1, 2, 3, 4],\n",
    "    'path': './results/2.2_Results/client_{0}/feature_{1}_fold_{2}_model.pth'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b5d34-d75b-48b9-a7d2-52a86a0cc712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55aeb134-0bf6-4f4f-8018-6cfc65266b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc834c8-24d6-466f-a742-4fbba263fb18",
   "metadata": {},
   "source": [
    "### 2.1 Accuracy/Loss vs Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f53caf3-4641-4fb3-a80e-5e6a70b42745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component in result_sources.get('components'):    \n",
    "#     loss_distributed = [] \n",
    "#     loss_centralized = [] \n",
    "#     accuracy_distributed =[] \n",
    "#     accuracy_centralized = []\n",
    "   \n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         history_path = result_sources.get('path').format(component, fold) + '/history.pkl'        \n",
    "#         l_d, l_c, a_d, a_c = parse_history(history_path)        \n",
    "#         loss_distributed.append(l_d)\n",
    "#         loss_centralized.append(l_c)\n",
    "#         accuracy_distributed.append(a_d)\n",
    "#         accuracy_centralized.append(a_c)\n",
    "\n",
    "#     history_plots = [\n",
    "#         {\n",
    "#             'type': 'distributed_loss',\n",
    "#             'plot_name': 'Distributed Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [0, 0],\n",
    "#             'data': loss_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'accuracy_distributed',\n",
    "#             'plot_name': 'Distributed Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [0, 1],\n",
    "#             'data': accuracy_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#          {\n",
    "#             'type': 'centralized_loss',\n",
    "#             'plot_name': 'Centralized Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [1, 0],\n",
    "#             'data': loss_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'centralized_accuracy',\n",
    "#             'plot_name': 'Centralized Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [1, 1],\n",
    "#             'data': accuracy_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },     \n",
    "      \n",
    "#     ]\n",
    "\n",
    "#     fig, ax = plt.subplots(2, 2, figsize=(16, 9))  # Adjust the figsize as needed\n",
    "#     for plot in  history_plots:\n",
    "#         position = plot.get('plot_position')\n",
    "#         all_fold_data = plot.get('data')\n",
    "#         #print(len(all_fold_data))\n",
    "#         for i, data in enumerate(all_fold_data):\n",
    "#             rounds = list(range(1, len(data)+1))\n",
    "#             ax[position[0], position[1]].plot(rounds, data, label=f'Fold_{i+1}', marker='o', color=plot.get('colors')[i])\n",
    "#             ax[position[0], position[1]].set_title(plot.get('plot_name').format(component))\n",
    "#             ax[position[0], position[1]].set_xlabel(plot.get('x'))\n",
    "#             ax[position[0], position[1]].set_ylabel(plot.get('y'))\n",
    "#             ax[position[0], position[1]].legend()\n",
    "#             ax[position[0], position[1]].grid(True)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331ad99-9689-4909-824a-f4921522fe14",
   "metadata": {},
   "source": [
    "### 2.2 Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2107e6-fb98-4588-8ffa-d206223fe877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##Plotting the Time charts\n",
    "# # for component in result_sources.get('components'):\n",
    "# #     training_time = []\n",
    "# #     for fold in result_sources.get('folds'):\n",
    "# #         ##Parsign the training time\n",
    "# #         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "# #         training_time = parse_training_time(training_time_path)\n",
    "# #         print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "# for component in result_sources.get('components'):\n",
    "#     training_time = []\n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         ##Parsign the training time\n",
    "#         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "#         training_time.append(parse_training_time(training_time_path))\n",
    "#         #print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "#     training_time_to_string = \", \".join(map(str, training_time))\n",
    "#     print(f'{component}, {training_time_to_string}')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b5db2-b662-4581-84fd-87faceaab836",
   "metadata": {},
   "source": [
    "## 3. Accumulate Results\n",
    "- Accumulate all thre results and save in csv file\n",
    "- It also stores values reauired for confusion matrix in a varialbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf6381e-b55e-4af1-ad87-c274d9c818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "client_metrics = {\n",
    "    'Component': [],\n",
    "    'Fold': [],\n",
    "    'Client': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1_Score': [],\n",
    "    'Sample_Number': [],\n",
    "    'Inference_Time_Per_Sample': []\n",
    "}\n",
    "\n",
    "classes = np.arange(NUM_CLASSES)  # Define or import this variable\n",
    "\n",
    "def accumulate_results(results, confusion_matrix_data):\n",
    "    components = results.get('components')\n",
    "    folds = results.get('folds')\n",
    "    path = results.get('path')\n",
    "    clients = results.get('clients')\n",
    "\n",
    "    for client in clients:    \n",
    "        for component in components:  \n",
    "            for fold in folds:\n",
    "                testset = get_evaluation_datasets_by_client(client, fold=fold, feature_count=component)  \n",
    "                testloader = DataLoader(to_tensor(testset), batch_size=BATCH_SIZE)                \n",
    "                \n",
    "                model_path = path.format(client, component, fold)                        \n",
    "                model = load_model(model_path=model_path, input_size=component, num_classes=NUM_CLASSES)\n",
    "                model.to(device)\n",
    "\n",
    "                preds, labels, inference_time_per_sample = run_inference(model, testloader, device)                                  \n",
    "                   \n",
    "                client_metrics['Component'].append(component)\n",
    "                client_metrics['Fold'].append(fold)\n",
    "                client_metrics['Client'].append(client)\n",
    "                client_metrics['Accuracy'].append(accuracy_score(labels, preds))\n",
    "                client_metrics['Precision'].append(precision_score(labels, preds))\n",
    "                client_metrics['Recall'].append(recall_score(labels, preds))\n",
    "                client_metrics['F1_Score'].append(f1_score(labels, preds))\n",
    "                client_metrics['Sample_Number'].append(len(labels)),\n",
    "                client_metrics['Inference_Time_Per_Sample'].append(inference_time_per_sample)\n",
    "\n",
    "                #Saving info for confusion matrix\n",
    "                key = f'{component}_{fold}_{client}'\n",
    "                confusion_matrix_data[key] = {\n",
    "                    'preds': preds,\n",
    "                    'labels': labels,\n",
    "                    'classes': np.arange(NUM_CLASSES)\n",
    "                }   \n",
    "\n",
    "    ##Converting into datafram for better visualization\n",
    "    df = pd.DataFrame(client_metrics)    \n",
    "    return df, confusion_matrix_data           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c6cb12-f99f-4019-9a45-c9c814003fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data = {}\n",
    "result_df, store_results_df = accumulate_results(result_sources, confusion_matrix_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec77a390-8c97-4699-aea1-8e0204041f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Component  Fold  Client  Accuracy  Precision   Recall  F1_Score  Sample_Number Inference_Time_Per_Sample\n",
      "         4     1       1  0.834617   0.947466 0.708524  0.810756          83999                18.4738 us\n",
      "         4     2       1  0.963869   0.932606 1.000000  0.965128          83999                 5.2938 us\n",
      "         4     3       1  0.832869   0.940928 0.710333  0.809530          84000                 5.3189 us\n",
      "         4     4       1  0.964060   0.932938 1.000000  0.965306          84000                 6.4410 us\n",
      "         4     5       1  0.965298   0.935100 1.000000  0.966461          84000                 5.3050 us\n",
      "         5     1       1  0.964595   0.933873 1.000000  0.965806          83999                 5.3475 us\n",
      "         5     2       1  0.834415   0.947037 0.708445  0.810548          83999                 5.3219 us\n",
      "         5     3       1  0.835119   0.946570 0.710333  0.811611          84000                 6.5024 us\n",
      "         5     4       1  0.834738   0.946514 0.709565  0.811088          84000                 5.3339 us\n",
      "         5     5       1  0.834774   0.945446 0.710548  0.811337          84000                 5.3638 us\n",
      "         6     1       1  0.967404   0.938799 1.000000  0.968434          83999                 5.4361 us\n",
      "         6     2       1  0.834415   0.947037 0.708445  0.810548          83999                 6.6083 us\n",
      "         6     3       1  0.966940   0.937982 1.000000  0.967998          84000                 5.4242 us\n",
      "         6     4       1  0.975667   0.953591 1.000000  0.976244          84000                 5.3840 us\n",
      "         6     5       1  0.975786   0.953808 1.000000  0.976358          84000                 5.3786 us\n",
      "         7     1       1  0.975988   0.954177 1.000000  0.976551          83999                 5.4028 us\n",
      "         7     2       1  0.965404   0.935286 1.000000  0.966561          83999                 6.5348 us\n",
      "         7     3       1  0.835012   0.946300 0.710333  0.811511          84000                 5.4096 us\n",
      "         7     4       1  0.975667   0.953591 1.000000  0.976244          84000                 5.3769 us\n",
      "         7     5       1  0.967762   0.939429 1.000000  0.968769          84000                 5.3942 us\n",
      "         8     1       1  0.834605   0.947435 0.708524  0.810745          83999                 6.7639 us\n",
      "         8     2       1  0.900523   0.946513 0.849020  0.895120          83999                 5.5950 us\n",
      "         8     3       1  0.835119   0.946570 0.710333  0.811611          84000                 5.5515 us\n",
      "         8     4       1  0.899833   0.945744 0.848330  0.894392          84000                 5.5149 us\n",
      "         8     5       1  0.899905   0.945990 0.848238  0.894451          84000                 6.7308 us\n",
      "         9     1       1  0.832129   0.941203 0.708524  0.808455          83999                 5.5854 us\n",
      "         9     2       1  0.853605   0.998255 0.708445  0.828745          83999                 5.5108 us\n",
      "         9     3       1  0.973310   0.949324 1.000000  0.974003          84000                 5.5746 us\n",
      "         9     4       1  0.899845   0.945769 0.848330  0.894403          84000                 5.5297 us\n",
      "         9     5       1  0.899893   0.945965 0.848238  0.894440          84000                 6.6679 us\n",
      "        10     1       1  0.897308   0.940709 0.848071  0.891991          83999                 5.6242 us\n",
      "        10     2       1  0.899606   0.944583 0.849020  0.894256          83999                 5.5741 us\n",
      "        10     3       1  0.899357   0.944036 0.849048  0.894026          84000                 5.5926 us\n",
      "        10     4       1  0.899905   0.945894 0.848330  0.894459          84000                 6.7300 us\n",
      "        10     5       1  0.974762   0.951949 1.000000  0.975383          84000                 5.5471 us\n",
      "        11     1       1  0.895189   0.936284 0.848095  0.890010          83999                 5.6129 us\n",
      "        11     2       1  0.900475   0.946413 0.849020  0.895075          83999                 5.5687 us\n",
      "        11     3       1  0.900607   0.946667 0.849048  0.895204          84000                 6.5614 us\n",
      "        11     4       1  0.899833   0.945744 0.848330  0.894392          84000                 5.5641 us\n",
      "        11     5       1  0.898310   0.942636 0.848238  0.892949          84000                 5.5946 us\n",
      "        12     1       1  0.898404   0.943000 0.848071  0.893020          83999                 5.6916 us\n",
      "        12     2       1  0.900594   0.946664 0.849020  0.895187          83999                 5.6268 us\n",
      "        12     3       1  0.973440   0.949560 1.000000  0.974128          84000                 6.6143 us\n",
      "        12     4       1  0.972798   0.948401 1.000000  0.973517          84000                 5.6372 us\n",
      "        12     5       1  0.976488   0.955088 1.000000  0.977028          84000                 5.6163 us\n",
      "        13     1       1  0.900046   0.946458 0.848071  0.894568          83999                 5.6999 us\n",
      "        13     2       1  0.973976   0.950526 1.000000  0.974636          83999                 6.6791 us\n",
      "        13     3       1  0.900571   0.946592 0.849048  0.895170          84000                 5.6622 us\n",
      "        13     4       1  0.899845   0.945769 0.848330  0.894403          84000                 5.6356 us\n",
      "        13     5       1  0.900690   0.947651 0.848238  0.895193          84000                 5.6511 us\n",
      "        14     1       1  0.901713   0.950016 0.848048  0.896140          83999                 6.6713 us\n",
      "        14     2       1  0.976666   0.955413 1.000000  0.977198          83999                 5.6640 us\n",
      "        14     3       1  0.869738   0.951267 0.779405  0.856803          84000                 5.6895 us\n",
      "        14     4       1  0.899381   0.944791 0.848330  0.893966          84000                 5.6543 us\n",
      "        14     5       1  0.923440   0.998403 0.848238  0.917215          84000                 6.6103 us\n",
      "        15     1       1  0.976083   0.954371 0.999976  0.976641          83999                 5.6972 us\n",
      "        15     2       1  0.900380   0.946212 0.849020  0.894985          83999                 5.6636 us\n",
      "        15     3       1  0.899869   0.945111 0.849048  0.894508          84000                 5.7271 us\n",
      "        15     4       1  0.899107   0.944215 0.848330  0.893708          84000                 5.6569 us\n",
      "        15     5       1  0.901048   0.948408 0.848238  0.895531          84000                 6.6589 us\n",
      "         4     1       2  0.869238   0.948909 0.780500  0.856505          84000                 5.3814 us\n",
      "         4     2       2  0.888475   0.996803 0.779452  0.874830          83999                 5.4621 us\n",
      "         4     3       2  0.889251   0.999115 0.779196  0.875557          84001                 5.4652 us\n",
      "         4     4       2  0.975703   0.953658 1.000000  0.976279          84001                 6.3999 us\n",
      "         4     5       2  0.976167   0.954502 1.000000  0.976721          84000                 5.4300 us\n",
      "         5     1       2  0.975667   0.953592 1.000000  0.976245          84000                 5.4287 us\n",
      "         5     2       2  0.976333   0.954806 1.000000  0.976880          83999                 5.4331 us\n",
      "         5     3       2  0.869763   0.951560 0.779196  0.856795          84001                 6.4107 us\n",
      "         5     4       2  0.887228   0.997188 0.776648  0.873208          84001                 5.4079 us\n",
      "         5     5       2  0.975417   0.953137 1.000000  0.976007          84000                 5.4291 us\n",
      "         6     1       2  0.889179   0.997262 0.780500  0.875666          84000                 5.4330 us\n",
      "         6     2       2  0.976428   0.954980 1.000000  0.976971          83999                 5.4511 us\n",
      "         6     3       2  0.976405   0.954937 1.000000  0.976949          84001                 6.3949 us\n",
      "         6     4       2  0.975750   0.953744 1.000000  0.976325          84001                 5.4432 us\n",
      "         6     5       2  0.966417   0.937061 1.000000  0.967508          84000                 5.4207 us\n",
      "         7     1       2  0.975643   0.953549 1.000000  0.976222          84000                 5.4525 us\n",
      "         7     2       2  0.976940   0.955914 1.000000  0.977460          83999                 6.6019 us\n",
      "         7     3       2  0.869537   0.951035 0.779196  0.856582          84001                 5.5215 us\n",
      "         7     4       2  0.975750   0.953744 1.000000  0.976325          84001                 5.4445 us\n",
      "         7     5       2  0.975143   0.952640 1.000000  0.975746          84000                 5.4716 us\n",
      "         8     1       2  0.900940   0.945831 0.850595  0.895689          84000                 6.7802 us\n",
      "         8     2       2  0.888856   0.997775 0.779452  0.875204          83999                 5.6279 us\n",
      "         8     3       2  0.923620   0.998459 0.848551  0.917422          84001                 5.6174 us\n",
      "         8     4       2  0.970762   0.944756 1.000000  0.971593          84001                 5.5806 us\n",
      "         8     5       2  0.999131   0.998265 1.000000  0.999132          84000                 5.5802 us\n",
      "         9     1       2  0.975631   0.953527 1.000000  0.976211          84000                 6.7098 us\n",
      "         9     2       2  0.976416   0.954958 1.000000  0.976960          83999                 5.5757 us\n",
      "         9     3       2  0.976357   0.954850 1.000000  0.976904          84001                 5.6102 us\n",
      "         9     4       2  0.965989   0.936310 1.000000  0.967108          84001                 5.5910 us\n",
      "         9     5       2  0.999155   0.998312 1.000000  0.999155          84000                 6.5489 us\n",
      "        10     1       2  0.953000   0.998611 0.907262  0.950747          84000                 5.6323 us\n",
      "        10     2       2  0.900939   0.947443 0.848976  0.895511          83999                 5.6189 us\n",
      "        10     3       2  0.964453   0.933625 1.000000  0.965673          84001                 6.0530 us\n",
      "        10     4       2  0.965596   0.935622 1.000000  0.966740          84001                 6.8075 us\n",
      "        10     5       2  0.976167   0.954502 1.000000  0.976721          84000                 5.6015 us\n",
      "        11     1       2  0.976167   0.954502 1.000000  0.976721          84000                 5.6145 us\n",
      "        11     2       2  0.976428   0.954980 1.000000  0.976971          83999                 5.6088 us\n",
      "        11     3       2  0.899989   0.945860 0.848551  0.894567          84001                 5.6729 us\n",
      "        11     4       2  0.970929   0.945053 1.000000  0.971750          84001                 6.5095 us\n",
      "        11     5       2  0.964488   0.933686 1.000000  0.965706          84000                 5.6172 us\n",
      "        12     1       2  0.953345   0.999371 0.907262  0.951091          84000                 5.6726 us\n",
      "        12     2       2  0.976381   0.954893 1.000000  0.976926          83999                 5.6843 us\n",
      "        12     3       2  0.901454   0.948958 0.848551  0.895950          84001                 6.6452 us\n",
      "        12     4       2  0.965619   0.935664 1.000000  0.966763          84001                 5.6813 us\n",
      "        12     5       2  0.899833   0.945485 0.848595  0.894424          84000                 5.6746 us\n",
      "        13     1       2  0.900869   0.945681 0.850595  0.895622          84000                 5.6790 us\n",
      "        13     2       2  0.953428   0.999004 0.907762  0.951200          83999                 6.8381 us\n",
      "        13     3       2  0.900620   0.947168 0.848575  0.895165          84001                 5.7147 us\n",
      "        13     4       2  0.964262   0.998517 0.929906  0.962991          84001                 5.6936 us\n",
      "        13     5       2  0.975417   0.953137 1.000000  0.976007          84000                 5.6870 us\n",
      "        14     1       2  0.877143   0.995310 0.757857  0.860503          84000                 5.6798 us\n",
      "        14     2       2  0.923082   0.996896 0.848810  0.916912          83999                 6.8215 us\n",
      "        14     3       2  0.951560   0.997404 0.905478  0.949221          84001                 5.7267 us\n",
      "        14     4       2  0.910858   0.997549 0.823742  0.902353          84001                 5.6993 us\n",
      "        14     5       2  0.950679   0.994023 0.906810  0.948416          84000                 5.6679 us\n",
      "        15     1       2  0.964393   0.998798 0.929905  0.963121          84000                 6.8344 us\n",
      "        15     2       2  0.911094   0.996349 0.825214  0.902743          83999                 5.7017 us\n",
      "        15     3       2  0.917787   0.908642 0.928978  0.918697          84001                 5.7066 us\n",
      "        15     4       2  0.876013   0.997856 0.753649  0.858728          84001                 5.6874 us\n",
      "        15     5       2  0.923393   0.997872 0.848595  0.917200          84000                 6.9831 us\n",
      "         4     1       3  0.998726   0.997459 1.000000  0.998728          83999                 5.8861 us\n",
      "         4     2       3  0.998262   0.996536 1.000000  0.998265          83999                 5.8684 us\n",
      "         4     3       3  0.998131   0.996276 1.000000  0.998134          84000                 5.8785 us\n",
      "         4     4       3  0.998500   0.997009 1.000000  0.998502          83999                 5.8738 us\n",
      "         4     5       3  0.998595   0.997198 1.000000  0.998597          84000                 6.8196 us\n",
      "         5     1       3  0.998679   0.997364 1.000000  0.998680          83999                 5.8349 us\n",
      "         5     2       3  0.998607   0.997222 1.000000  0.998609          83999                 5.8523 us\n",
      "         5     3       3  0.998524   0.997056 1.000000  0.998526          84000                 5.8535 us\n",
      "         5     4       3  0.998476   0.996962 1.000000  0.998479          83999                 6.7809 us\n",
      "         5     5       3  0.998417   0.996843 1.000000  0.998419          84000                 5.9127 us\n",
      "         6     1       3  0.998726   0.997459 1.000000  0.998728          83999                 5.8091 us\n",
      "         6     2       3  0.998560   0.997127 1.000000  0.998562          83999                 5.8969 us\n",
      "         6     3       3  0.998524   0.997056 1.000000  0.998526          84000                 6.8246 us\n",
      "         6     4       3  0.998452   0.996914 1.000000  0.998455          83999                 5.8690 us\n",
      "         6     5       3  0.998440   0.996891 1.000000  0.998443          84000                 5.7804 us\n",
      "         7     1       3  0.998679   0.997364 1.000000  0.998680          83999                 5.8599 us\n",
      "         7     2       3  0.998762   0.997530 1.000000  0.998763          83999                 5.8537 us\n",
      "         7     3       3  0.998714   0.997435 1.000000  0.998716          84000                 6.8803 us\n",
      "         7     4       3  0.990869   0.982066 1.000000  0.990952          83999                 5.8855 us\n",
      "         7     5       3  0.998357   0.996725 1.000000  0.998360          84000                 5.8789 us\n",
      "         8     1       3  0.991333   0.982962 1.000000  0.991408          83999                 6.0450 us\n",
      "         8     2       3  0.999024   0.998051 1.000000  0.999025          83999                 7.0341 us\n",
      "         8     3       3  0.999048   0.998099 1.000000  0.999049          84000                 6.0565 us\n",
      "         8     4       3  0.991428   0.983146 1.000000  0.991501          83999                 6.0476 us\n",
      "         8     5       3  0.999012   0.998028 1.000000  0.999013          84000                 6.0598 us\n",
      "         9     1       3  0.999048   0.998099 1.000000  0.999049          83999                 7.0419 us\n",
      "         9     2       3  0.990321   0.981010 1.000000  0.990414          83999                 6.0694 us\n",
      "         9     3       3  0.998952   0.997909 1.000000  0.998954          84000                 6.0276 us\n",
      "         9     4       3  0.999083   0.998170 1.000000  0.999084          83999                 6.0926 us\n",
      "         9     5       3  0.999012   0.998028 1.000000  0.999013          84000                 6.0793 us\n",
      "        10     1       3  0.985714   0.972222 1.000000  0.985915          83999                 7.0153 us\n",
      "        10     2       3  0.999024   0.998051 1.000000  0.999025          83999                 6.0767 us\n",
      "        10     3       3  0.999048   0.998099 1.000000  0.999049          84000                 6.1058 us\n",
      "        10     4       3  0.999083   0.998170 1.000000  0.999084          83999                 6.0584 us\n",
      "        10     5       3  0.999000   0.998004 1.000000  0.999001          84000                 7.0553 us\n",
      "        11     1       3  0.987905   0.976381 1.000000  0.988049          83999                 6.1310 us\n",
      "        11     2       3  0.999024   0.998051 1.000000  0.999025          83999                 6.1095 us\n",
      "        11     3       3  0.998714   0.997435 1.000000  0.998716          84000                 6.1176 us\n",
      "        11     4       3  0.992190   0.984621 1.000000  0.992251          83999                 7.0744 us\n",
      "        11     5       3  0.992381   0.984991 1.000000  0.992439          84000                 6.1076 us\n",
      "        12     1       3  0.999048   0.998099 1.000000  0.999049          83999                 6.1957 us\n",
      "        12     2       3  0.997595   0.995213 1.000000  0.997601          83999                 6.1664 us\n",
      "        12     3       3  0.999048   0.998099 1.000000  0.999049          84000                 7.1975 us\n",
      "        12     4       3  0.963190   0.931429 1.000000  0.964497          83999                 6.1440 us\n",
      "        12     5       3  0.999012   0.998028 1.000000  0.999013          84000                 6.1889 us\n",
      "        13     1       3  0.981309   0.963966 1.000000  0.981652          83999                 6.1906 us\n",
      "        13     2       3  0.999024   0.998051 1.000000  0.999025          83999                 6.1284 us\n",
      "        13     3       3  0.991107   0.982525 1.000000  0.991186          84000                 7.0939 us\n",
      "        13     4       3  0.999083   0.998170 1.000000  0.999084          83999                 6.1397 us\n",
      "        13     5       3  0.991000   0.982318 1.000000  0.991080          84000                 6.2833 us\n",
      "        14     1       3  0.999083   0.998170 1.000000  0.999084          83999                 6.1692 us\n",
      "        14     2       3  0.999440   0.998882 1.000000  0.999441          83999                 7.1644 us\n",
      "        14     3       3  0.999107   0.998218 1.000000  0.999108          84000                 6.1836 us\n",
      "        14     4       3  0.999167   0.998336 1.000000  0.999167          83999                 6.1534 us\n",
      "        14     5       3  0.995667   0.991408 1.000000  0.995685          84000                 6.1535 us\n",
      "        15     1       3  0.999119   0.998241 1.000000  0.999120          83999                 7.1558 us\n",
      "        15     2       3  0.958845   0.923950 1.000000  0.960472          83999                 6.1489 us\n",
      "        15     3       3  0.999119   0.998241 1.000000  0.999120          84000                 6.1598 us\n",
      "        15     4       3  0.999202   0.998407 1.000000  0.999203          83999                 6.1329 us\n",
      "        15     5       3  0.999060   0.998123 1.000000  0.999060          84000                 6.1496 us\n",
      "         4     1       4  0.899489   0.999197 0.799624  0.888339          84001                 6.8074 us\n",
      "         4     2       4  0.900404   0.998695 0.801852  0.889514          83999                 5.8430 us\n",
      "         4     3       4  0.899961   0.998694 0.800971  0.888971          83997                 5.8589 us\n",
      "         4     4       4  0.900392   0.998429 0.802048  0.889529          83999                 5.8893 us\n",
      "         4     5       4  0.901404   0.999053 0.803571  0.890713          83999                 6.8213 us\n",
      "         5     1       4  0.899275   0.998662 0.799624  0.888128          84001                 5.8240 us\n",
      "         5     2       4  0.891844   0.977846 0.801852  0.881147          83999                 5.8216 us\n",
      "         5     3       4  0.900044   0.998901 0.800971  0.889053          83997                 5.8734 us\n",
      "         5     4       4  0.900630   0.999021 0.802048  0.889763          83999                 6.8584 us\n",
      "         5     5       4  0.901308   0.998816 0.803571  0.890619          83999                 5.8997 us\n",
      "         6     1       4  0.936298   0.994602 0.877360  0.932310          84001                 5.8891 us\n",
      "         6     2       4  0.898296   0.993480 0.801852  0.887440          83999                 5.8806 us\n",
      "         6     3       4  0.897187   0.991833 0.800971  0.886243          83997                 5.9003 us\n",
      "         6     4       4  0.937154   0.993575 0.880000  0.933345          83999                 6.8807 us\n",
      "         6     5       4  0.937106   0.994159 0.879381  0.933254          83999                 5.9096 us\n",
      "         7     1       4  0.934548   0.990671 0.877360  0.930579          84001                 5.9292 us\n",
      "         7     2       4  0.898380   0.993686 0.801852  0.887521          83999                 5.8978 us\n",
      "         7     3       4  0.935641   0.993021 0.877449  0.931665          83997                 6.8983 us\n",
      "         7     4       4  0.937071   0.993388 0.880000  0.933263          83999                 5.8900 us\n",
      "         7     5       4  0.937261   0.994507 0.879381  0.933407          83999                 5.9100 us\n",
      "         8     1       4  0.936096   0.994146 0.877360  0.932109          84001                 6.0626 us\n",
      "         8     2       4  0.929059   0.978466 0.877426  0.925196          83999                 7.0669 us\n",
      "         8     3       4  0.936045   0.993932 0.877449  0.932065          83997                 6.0267 us\n",
      "         8     4       4  0.930618   0.979125 0.880000  0.926920          83999                 6.0008 us\n",
      "         8     5       4  0.899201   0.993611 0.803571  0.888544          83999                 5.9876 us\n",
      "         9     1       4  0.936298   0.994602 0.877360  0.932310          84001                 5.9777 us\n",
      "         9     2       4  0.929832   0.980158 0.877426  0.925951          83999                 6.9785 us\n",
      "         9     3       4  0.899711   0.998072 0.800971  0.888725          83997                 6.0391 us\n",
      "         9     4       4  0.937714   0.994832 0.880000  0.933899          83999                 5.9753 us\n",
      "         9     5       4  0.930190   0.978851 0.879381  0.926454          83999                 5.9816 us\n",
      "        10     1       4  0.936298   0.994602 0.877360  0.932310          84001                 7.0523 us\n",
      "        10     2       4  0.936285   0.994495 0.877426  0.932299          83999                 6.0917 us\n",
      "        10     3       4  0.936379   0.994683 0.877449  0.932396          83997                 5.6850 us\n",
      "        10     4       4  0.937368   0.994056 0.880000  0.933557          83999                 5.6469 us\n",
      "        10     5       4  0.937261   0.994507 0.879381  0.933407          83999                 6.8073 us\n",
      "        11     1       4  0.936298   0.994602 0.877360  0.932310          84001                 5.6212 us\n",
      "        11     2       4  0.932737   0.986561 0.877426  0.928798          83999                 5.6508 us\n",
      "        11     3       4  0.936033   0.993905 0.877449  0.932054          83997                 5.6957 us\n",
      "        11     4       4  0.930499   0.978865 0.880000  0.926804          83999                 5.6603 us\n",
      "        11     5       4  0.937261   0.994507 0.879381  0.933407          83999                 6.7393 us\n",
      "        12     1       4  0.903454   0.984476 0.819838  0.894645          84001                 5.6985 us\n",
      "        12     2       4  0.936106   0.994092 0.877426  0.932123          83999                 5.7253 us\n",
      "        12     3       4  0.936379   0.994683 0.877449  0.932396          83997                 5.7453 us\n",
      "        12     4       4  0.903749   0.994402 0.812071  0.894035          83999                 6.8456 us\n",
      "        12     5       4  0.930225   0.978929 0.879381  0.926488          83999                 5.7194 us\n",
      "        13     1       4  0.933322   0.987936 0.877360  0.929370          84001                 5.6900 us\n",
      "        13     2       4  0.936261   0.994441 0.877426  0.932276          83999                 5.6710 us\n",
      "        13     3       4  0.902901   0.993410 0.811186  0.893098          83997                 6.9367 us\n",
      "        13     4       4  0.937380   0.994083 0.880000  0.933569          83999                 5.6957 us\n",
      "        13     5       4  0.930344   0.979188 0.879381  0.926605          83999                 5.7066 us\n",
      "        14     1       4  0.898275   0.984532 0.809266  0.888337          84001                 5.7062 us\n",
      "        14     2       4  0.988571   0.977653 1.000000  0.988700          83999                 5.7256 us\n",
      "        14     3       4  0.932212   0.985374 0.877449  0.928285          83997                 6.8971 us\n",
      "        14     4       4  0.899630   0.984471 0.812071  0.889999          83999                 5.7256 us\n",
      "        14     5       4  0.904308   0.994410 0.813190  0.894716          83999                 5.7147 us\n",
      "        15     1       4  0.898239   0.984447 0.809266  0.888302          84001                 5.7055 us\n",
      "        15     2       4  0.903130   0.993990 0.811162  0.893317          83999                 6.9252 us\n",
      "        15     3       4  0.903354   0.994512 0.811186  0.893543          83997                 5.7377 us\n",
      "        15     4       4  0.899439   0.984017 0.812071  0.889814          83999                 5.7102 us\n",
      "        15     5       4  0.904975   0.996034 0.813190  0.895373          83999                 5.7011 us\n"
     ]
    }
   ],
   "source": [
    "result_df.to_csv(\"./results/2.2_Results/2.2_uk25_noFL_results.csv\", index=False)\n",
    "print(result_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee64a5-4404-4e51-aed9-cbcc52e1d352",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix (per client per Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369edecd-71ed-4395-9f49-15e6138f84fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860421bd-e59e-4d79-be72-7bc5aa089873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    {\n",
    "        'client_id': 1,\n",
    "        'plot_name': 'Client 1',\n",
    "        'plot_position': [0, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 2,\n",
    "        'plot_name': 'Client 2',\n",
    "        'plot_position': [0, 1]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 3,\n",
    "        'plot_name': 'Client 3',\n",
    "        'plot_position': [1, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 4,\n",
    "        'plot_name': 'Client 4',\n",
    "        'plot_position': [1, 1]\n",
    "    }   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd872bc-f4dd-400d-9546-7751662492bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
