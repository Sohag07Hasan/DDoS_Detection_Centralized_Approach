{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424c0338-ba52-45c7-b636-7c05ff16352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from dataloader import get_evaluation_datasets_by_client  # Assuming this function gets local client datasets\n",
    "from model import Net\n",
    "from collections import OrderedDict\n",
    "from config import NUM_CLASSES, MODEL_PATH, BATCH_SIZE\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import to_tensor\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef994e1c-6907-4e54-8e2a-e623db313884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ad0a5-4b91-474f-97ef-a0f36ceacc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de8f59b-1122-4aab-aec3-84239c5baf44",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5a2eaf-1391-4c5e-a4cb-ebc40517757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the global model from the saved path\n",
    "def load_model(input_size, num_classes=NUM_CLASSES, model_path=MODEL_PATH):\n",
    "    model = Net(input_size=input_size, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e4f19d-02cd-42b4-a9ca-97dadfb76ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a client's dataset\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Get the total number of samples from the DataLoader\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    # Start the timer before the loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # End the timer after the loop\n",
    "    end_time = time.time()    \n",
    "    # Calculate the total inference time\n",
    "    total_inference_time = end_time - start_time    \n",
    "    # Calculate average inference time per sample\n",
    "    #inference_time_per_sample =  total_inference_time * 1000\n",
    "    inference_time_per_sample =  total_inference_time * 1000000 / total_samples\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), f'{inference_time_per_sample:.4f} us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106d4a49-75bd-4cc1-b995-0fdc5e26b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional: Set a title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59336a77-e75c-4299-9b8b-d9be081b3510",
   "metadata": {},
   "source": [
    "## 2. Performance/History of Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bf58d5-b6f7-4295-8907-44d74e8bdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sources = {\n",
    "    'components': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
    "    'folds': [1, 2, 3, 4, 5],\n",
    "    'marker': ['o', '-', '^' 'x', '-o-'],\n",
    "    'clients': [1, 2, 3, 4],\n",
    "    'path': './results/client_{0}/feature_{1}_fold_{2}_model.pth'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b5d34-d75b-48b9-a7d2-52a86a0cc712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55aeb134-0bf6-4f4f-8018-6cfc65266b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc834c8-24d6-466f-a742-4fbba263fb18",
   "metadata": {},
   "source": [
    "### 2.1 Accuracy/Loss vs Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53caf3-4641-4fb3-a80e-5e6a70b42745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component in result_sources.get('components'):    \n",
    "#     loss_distributed = [] \n",
    "#     loss_centralized = [] \n",
    "#     accuracy_distributed =[] \n",
    "#     accuracy_centralized = []\n",
    "   \n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         history_path = result_sources.get('path').format(component, fold) + '/history.pkl'        \n",
    "#         l_d, l_c, a_d, a_c = parse_history(history_path)        \n",
    "#         loss_distributed.append(l_d)\n",
    "#         loss_centralized.append(l_c)\n",
    "#         accuracy_distributed.append(a_d)\n",
    "#         accuracy_centralized.append(a_c)\n",
    "\n",
    "#     history_plots = [\n",
    "#         {\n",
    "#             'type': 'distributed_loss',\n",
    "#             'plot_name': 'Distributed Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [0, 0],\n",
    "#             'data': loss_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'accuracy_distributed',\n",
    "#             'plot_name': 'Distributed Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [0, 1],\n",
    "#             'data': accuracy_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#          {\n",
    "#             'type': 'centralized_loss',\n",
    "#             'plot_name': 'Centralized Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [1, 0],\n",
    "#             'data': loss_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'centralized_accuracy',\n",
    "#             'plot_name': 'Centralized Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [1, 1],\n",
    "#             'data': accuracy_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },     \n",
    "      \n",
    "#     ]\n",
    "\n",
    "#     fig, ax = plt.subplots(2, 2, figsize=(16, 9))  # Adjust the figsize as needed\n",
    "#     for plot in  history_plots:\n",
    "#         position = plot.get('plot_position')\n",
    "#         all_fold_data = plot.get('data')\n",
    "#         #print(len(all_fold_data))\n",
    "#         for i, data in enumerate(all_fold_data):\n",
    "#             rounds = list(range(1, len(data)+1))\n",
    "#             ax[position[0], position[1]].plot(rounds, data, label=f'Fold_{i+1}', marker='o', color=plot.get('colors')[i])\n",
    "#             ax[position[0], position[1]].set_title(plot.get('plot_name').format(component))\n",
    "#             ax[position[0], position[1]].set_xlabel(plot.get('x'))\n",
    "#             ax[position[0], position[1]].set_ylabel(plot.get('y'))\n",
    "#             ax[position[0], position[1]].legend()\n",
    "#             ax[position[0], position[1]].grid(True)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331ad99-9689-4909-824a-f4921522fe14",
   "metadata": {},
   "source": [
    "### 2.2 Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2107e6-fb98-4588-8ffa-d206223fe877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##Plotting the Time charts\n",
    "# # for component in result_sources.get('components'):\n",
    "# #     training_time = []\n",
    "# #     for fold in result_sources.get('folds'):\n",
    "# #         ##Parsign the training time\n",
    "# #         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "# #         training_time = parse_training_time(training_time_path)\n",
    "# #         print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "# for component in result_sources.get('components'):\n",
    "#     training_time = []\n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         ##Parsign the training time\n",
    "#         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "#         training_time.append(parse_training_time(training_time_path))\n",
    "#         #print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "#     training_time_to_string = \", \".join(map(str, training_time))\n",
    "#     print(f'{component}, {training_time_to_string}')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b5db2-b662-4581-84fd-87faceaab836",
   "metadata": {},
   "source": [
    "## 3. Accumulate Results\n",
    "- Accumulate all thre results and save in csv file\n",
    "- It also stores values reauired for confusion matrix in a varialbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf6381e-b55e-4af1-ad87-c274d9c818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "client_metrics = {\n",
    "    'Component': [],\n",
    "    'Fold': [],\n",
    "    'Client': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1_Score': [],\n",
    "    'Sample_Number': [],\n",
    "    'Inference_Time_Per_Sample': []\n",
    "}\n",
    "\n",
    "classes = np.arange(NUM_CLASSES)  # Define or import this variable\n",
    "\n",
    "def accumulate_results(results, confusion_matrix_data):\n",
    "    components = results.get('components')\n",
    "    folds = results.get('folds')\n",
    "    path = results.get('path')\n",
    "    clients = results.get('clients')\n",
    "\n",
    "    for client in clients:    \n",
    "        for component in components:  \n",
    "            for fold in folds:\n",
    "                testset = get_evaluation_datasets_by_client(client, fold=fold, feature_count=component)  \n",
    "                testloader = DataLoader(to_tensor(testset), batch_size=BATCH_SIZE)                \n",
    "                \n",
    "                model_path = path.format(client, component, fold)                        \n",
    "                model = load_model(model_path=model_path, input_size=component, num_classes=NUM_CLASSES)\n",
    "                model.to(device)\n",
    "\n",
    "                preds, labels, inference_time_per_sample = run_inference(model, testloader, device)                                  \n",
    "                   \n",
    "                client_metrics['Component'].append(component)\n",
    "                client_metrics['Fold'].append(fold)\n",
    "                client_metrics['Client'].append(client)\n",
    "                client_metrics['Accuracy'].append(accuracy_score(labels, preds))\n",
    "                client_metrics['Precision'].append(precision_score(labels, preds))\n",
    "                client_metrics['Recall'].append(recall_score(labels, preds))\n",
    "                client_metrics['F1_Score'].append(f1_score(labels, preds))\n",
    "                client_metrics['Sample_Number'].append(len(labels)),\n",
    "                client_metrics['Inference_Time_Per_Sample'].append(inference_time_per_sample)\n",
    "\n",
    "                #Saving info for confusion matrix\n",
    "                key = f'{component}_{fold}_{client}'\n",
    "                confusion_matrix_data[key] = {\n",
    "                    'preds': preds,\n",
    "                    'labels': labels,\n",
    "                    'classes': np.arange(NUM_CLASSES)\n",
    "                }   \n",
    "\n",
    "    ##Converting into datafram for better visualization\n",
    "    df = pd.DataFrame(client_metrics)\n",
    "    print(df.to_string(index=False))\n",
    "    return df, confusion_matrix_data           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c6cb12-f99f-4019-9a45-c9c814003fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Component  Fold  Client  Accuracy  Precision   Recall  F1_Score  Sample_Number Inference_Time_Per_Sample\n",
      "        12     1       1  0.591893   0.558900 1.000000  0.717044          84000                 6.6278 us\n",
      "        12     2       1  0.673333   0.612845 1.000000  0.759955          84000                 5.4556 us\n",
      "        12     3       1  0.806440   0.727635 0.999977  0.842340          84000                 5.4828 us\n",
      "        12     4       1  0.839810   0.763478 1.000000  0.865877          84000                 5.4827 us\n",
      "        12     5       1  0.718024   0.647130 0.999977  0.785760          84000                 5.4976 us\n",
      "        13     1       1  0.731655   0.658351 1.000000  0.793982          84000                 6.7375 us\n",
      "        13     2       1  0.862690   0.790176 1.000000  0.882792          84000                 5.4905 us\n",
      "        13     3       1  0.574929   0.548832 0.999977  0.708698          84000                 5.5341 us\n",
      "        13     4       1  0.621083   0.577102 1.000000  0.731851          84000                 5.4975 us\n",
      "        13     5       1  0.674583   0.613763 0.999977  0.760654          84000                 5.5048 us\n",
      "        14     1       1  0.970155   0.945432 1.000000  0.971951          84000                 6.7613 us\n",
      "        14     2       1  0.970012   0.945186 1.000000  0.971820          84000                 5.4838 us\n",
      "        14     3       1  0.970774   0.946522 0.999977  0.972515          84000                 5.5074 us\n",
      "        14     4       1  0.954643   0.919357 1.000000  0.957984          84000                 5.4769 us\n",
      "        14     5       1  0.970036   0.945247 0.999977  0.971842          84000                 6.7443 us\n",
      "        15     1       1  0.971333   0.947474 1.000000  0.973029          84000                 5.5440 us\n",
      "        15     2       1  0.970452   0.945947 1.000000  0.972223          84000                 5.5078 us\n",
      "        15     3       1  0.971107   0.947100 0.999977  0.972820          84000                 5.4943 us\n",
      "        15     4       1  0.971929   0.948507 1.000000  0.973573          84000                 5.4953 us\n",
      "        15     5       1  0.970369   0.945824 0.999977  0.972147          84000                 6.7785 us\n",
      "        16     1       1  0.970869   0.946669 1.000000  0.972604          84000                 5.5528 us\n",
      "        16     2       1  0.888369   0.884509 0.901879  0.893109          84000                 5.5187 us\n",
      "        16     3       1  0.722631   0.650868 1.000000  0.788516          84000                 5.4789 us\n",
      "        16     4       1  0.972179   0.948943 1.000000  0.973803          84000                 6.7413 us\n",
      "        16     5       1  0.970179   0.945494 0.999977  0.971973          84000                 5.5174 us\n",
      "        17     1       1  0.970393   0.945844 1.000000  0.972168          84000                 5.5478 us\n",
      "        17     2       1  0.969798   0.944815 1.000000  0.971625          84000                 5.5275 us\n",
      "        17     3       1  0.971143   0.947162 0.999977  0.972853          84000                 5.5011 us\n",
      "        17     4       1  0.898488   0.901814 0.901876  0.901845          84000                 6.7763 us\n",
      "        17     5       1  0.970119   0.945391 0.999977  0.971918          84000                 5.5168 us\n",
      "        18     1       1  0.975810   0.955309 1.000000  0.977144          84000                 6.0617 us\n",
      "        18     2       1  0.970286   0.945659 1.000000  0.972071          84000                 5.4887 us\n",
      "        18     3       1  0.974786   0.953525 0.999977  0.976199          84000                 5.4903 us\n",
      "        18     4       1  0.972286   0.949129 1.000000  0.973901          84000                 6.7082 us\n",
      "        18     5       1  0.970298   0.945700 0.999977  0.972081          84000                 5.4926 us\n",
      "        19     1       1  0.976036   0.955709 1.000000  0.977353          84000                 5.5773 us\n",
      "        19     2       1  0.974536   0.953066 1.000000  0.975969          84000                 5.5088 us\n",
      "        19     3       1  0.518536   0.517835 1.000000  0.682334          84000                 6.7775 us\n",
      "        19     4       1  0.972548   0.949586 1.000000  0.974141          84000                 5.5296 us\n",
      "        19     5       1  0.970071   0.945290 1.000000  0.971875          84000                 5.4989 us\n",
      "        20     1       1  0.977190   0.957753 1.000000  0.978421          84000                 5.5771 us\n",
      "        20     2       1  0.881929   0.814110 1.000000  0.897531          84000                 5.5002 us\n",
      "        20     3       1  0.970786   0.946543 0.999977  0.972526          84000                 6.7277 us\n",
      "        20     4       1  0.972155   0.948901 1.000000  0.973781          84000                 5.4807 us\n",
      "        20     5       1  0.975440   0.954679 0.999977  0.976803          84000                 5.5095 us\n",
      "        21     1       1  0.977060   0.957520 1.000000  0.978299          84000                 5.5641 us\n",
      "        21     2       1  0.974738   0.953422 1.000000  0.976156          84000                 6.7127 us\n",
      "        21     3       1  0.522250   0.519769 1.000000  0.684010          84000                 5.5697 us\n",
      "        21     4       1  0.976690   0.956866 1.000000  0.977957          84000                 5.5073 us\n",
      "        21     5       1  0.970595   0.946215 0.999977  0.972353          84000                 5.5338 us\n",
      "        22     1       1  0.977369   0.958070 1.000000  0.978586          84000                 5.5623 us\n",
      "        22     2       1  0.977095   0.957584 1.000000  0.978332          84000                 6.5392 us\n",
      "        22     3       1  0.975976   0.955623 0.999977  0.977297          84000                 5.5349 us\n",
      "        22     4       1  0.979071   0.961100 1.000000  0.980164          84000                 5.5065 us\n",
      "        22     5       1  0.977071   0.957563 0.999977  0.978310          84000                 5.5801 us\n",
      "        23     1       1  0.975869   0.955414 1.000000  0.977199          84000                 5.5568 us\n",
      "        23     2       1  0.975548   0.954847 1.000000  0.976902          84000                 6.7567 us\n",
      "        23     3       1  0.976833   0.957139 0.999977  0.978089          84000                 5.5057 us\n",
      "        23     4       1  0.978250   0.959635 1.000000  0.979402          84000                 5.5407 us\n",
      "        23     5       1  0.970369   0.945824 0.999977  0.972147          84000                 5.5353 us\n",
      "        24     1       1  0.976893   0.957225 1.000000  0.978145          84000                 6.8059 us\n",
      "        24     2       1  0.976726   0.956930 1.000000  0.977991          84000                 5.5195 us\n",
      "        24     3       1  0.976881   0.957223 0.999977  0.978133          84000                 5.5059 us\n",
      "        24     4       1  0.972107   0.948818 1.000000  0.973737          84000                 5.5578 us\n",
      "        24     5       1  0.977940   0.959106 0.999977  0.979115          84000                 5.5181 us\n",
      "        12     1       2  0.642845   0.609428 1.000000  0.757323          84000                 6.7128 us\n",
      "        12     2       2  0.644690   0.613894 0.976715  0.753925          84000                 5.5657 us\n",
      "        12     3       2  0.558321   0.557858 1.000000  0.716186          84000                 5.5751 us\n",
      "        12     4       2  0.604833   0.585101 1.000000  0.738251          84000                 5.5709 us\n",
      "        12     5       2  0.639905   0.607475 1.000000  0.755812          84000                 6.7395 us\n",
      "        13     1       2  0.557286   0.557286 1.000000  0.715714          84000                 5.5523 us\n",
      "        13     2       2  0.699619   0.671662 0.901839  0.769915          84000                 5.5135 us\n",
      "        13     3       2  0.650286   0.614422 1.000000  0.761167          84000                 5.5338 us\n",
      "        13     4       2  0.557619   0.557466 1.000000  0.715863          84000                 5.6420 us\n",
      "        13     5       2  0.620952   0.595179 1.000000  0.746222          84000                 6.4773 us\n",
      "        14     1       2  0.967226   0.944457 1.000000  0.971435          84000                 5.5526 us\n",
      "        14     2       2  0.972917   0.953653 1.000000  0.976277          84000                 5.5083 us\n",
      "        14     3       2  0.972976   0.953750 1.000000  0.976328          84000                 5.5482 us\n",
      "        14     4       2  0.972524   0.953012 1.000000  0.975941          84000                 5.5677 us\n",
      "        14     5       2  0.971702   0.951676 1.000000  0.975240          84000                 6.7103 us\n",
      "        15     1       2  0.971810   0.951850 1.000000  0.975331          84000                 5.6717 us\n",
      "        15     2       2  0.972702   0.953303 1.000000  0.976093          84000                 5.5289 us\n",
      "        15     3       2  0.972833   0.953517 1.000000  0.976205          84000                 5.5697 us\n",
      "        15     4       2  0.972631   0.953187 1.000000  0.976032          84000                 6.7963 us\n",
      "        15     5       2  0.972524   0.953013 1.000000  0.975941          84000                 5.6857 us\n",
      "        16     1       2  0.971393   0.951173 1.000000  0.974976          84000                 5.5342 us\n",
      "        16     2       2  0.565250   0.561754 1.000000  0.719389          84000                 5.5417 us\n",
      "        16     3       2  0.972440   0.952876 1.000000  0.975870          84000                 5.6596 us\n",
      "        16     4       2  0.972202   0.952489 1.000000  0.975666          84000                 6.7739 us\n",
      "        16     5       2  0.972881   0.953595 1.000000  0.976247          84000                 5.5856 us\n",
      "        17     1       2  0.971952   0.952083 1.000000  0.975453          84000                 5.5701 us\n",
      "        17     2       2  0.971667   0.951617 1.000000  0.975209          84000                 5.5448 us\n",
      "        17     3       2  0.972548   0.953051 1.000000  0.975961          84000                 6.7208 us\n",
      "        17     4       2  0.969857   0.948686 1.000000  0.973667          84000                 5.5318 us\n",
      "        17     5       2  0.973071   0.953906 1.000000  0.976409          84000                 5.5687 us\n",
      "        18     1       2  0.972726   0.953343 1.000000  0.976114          84000                 5.5220 us\n",
      "        18     2       2  0.973202   0.954119 1.000000  0.976521          84000                 5.9192 us\n",
      "        18     3       2  0.973821   0.955132 1.000000  0.977051          84000                 6.9340 us\n",
      "        18     4       2  0.557333   0.557307 1.000000  0.715732          84000                 5.5811 us\n",
      "        18     5       2  0.971881   0.951966 1.000000  0.975392          84000                 5.5547 us\n",
      "        19     1       2  0.975810   0.958398 1.000000  0.978757          84000                 5.5377 us\n",
      "        19     2       2  0.973452   0.954528 1.000000  0.976735          84000                 5.5940 us\n",
      "        19     3       2  0.973083   0.953925 1.000000  0.976419          84000                 6.7333 us\n",
      "        19     4       2  0.978179   0.962318 1.000000  0.980797          84000                 5.5129 us\n",
      "        19     5       2  0.973024   0.953829 1.000000  0.976369          84000                 5.5777 us\n",
      "        20     1       2  0.971738   0.951734 1.000000  0.975270          84000                 5.5140 us\n",
      "        20     2       2  0.973036   0.953847 1.000000  0.976378          84000                 6.7137 us\n",
      "        20     3       2  0.976762   0.959970 1.000000  0.979576          84000                 5.5218 us\n",
      "        20     4       2  0.977107   0.960541 1.000000  0.979873          84000                 5.5598 us\n",
      "        20     5       2  0.977345   0.960936 1.000000  0.980079          84000                 5.5562 us\n",
      "        21     1       2  0.975536   0.957947 1.000000  0.978522          84000                 5.5249 us\n",
      "        21     2       2  0.979476   0.964479 1.000000  0.981918          84000                 6.5526 us\n",
      "        21     3       2  0.973583   0.954742 1.000000  0.976847          84000                 5.5296 us\n",
      "        21     4       2  0.972893   0.953614 1.000000  0.976256          84000                 5.5328 us\n",
      "        21     5       2  0.973119   0.953984 1.000000  0.976450          84000                 5.5223 us\n",
      "        22     1       2  0.653667   0.616726 1.000000  0.762932          84000                 6.5415 us\n",
      "        22     2       2  0.980274   0.965812 1.000000  0.982609          84000                 5.5424 us\n",
      "        22     3       2  0.979190   0.964003 1.000000  0.981671          84000                 5.5519 us\n",
      "        22     4       2  0.979083   0.963824 1.000000  0.981579          84000                 5.5721 us\n",
      "        22     5       2  0.978512   0.962873 1.000000  0.981085          84000                 5.5283 us\n",
      "        23     1       2  0.977690   0.961508 1.000000  0.980377          84000                 6.7724 us\n",
      "        23     2       2  0.978167   0.962298 1.000000  0.980787          84000                 5.5051 us\n",
      "        23     3       2  0.977286   0.960837 1.000000  0.980027          84000                 5.6396 us\n",
      "        23     4       2  0.973048   0.953867 1.000000  0.976389          84000                 5.5378 us\n",
      "        23     5       2  0.978369   0.962635 1.000000  0.980962          84000                 6.7669 us\n",
      "        24     1       2  0.978452   0.962793 0.999979  0.981034          84000                 5.5840 us\n",
      "        24     2       2  0.978476   0.962813 1.000000  0.981054          84000                 5.5882 us\n",
      "        24     3       2  0.949083   0.916282 1.000000  0.956312          84000                 5.5709 us\n",
      "        24     4       2  0.973048   0.953867 1.000000  0.976389          84000                 5.5482 us\n",
      "        24     5       2  0.979238   0.964083 1.000000  0.981713          84000                 6.8178 us\n",
      "        12     1       3  0.760286   0.683768 0.968476  0.801592          84000                 5.5658 us\n",
      "        12     2       3  0.776381   0.698590 0.972238  0.813005          84000                 5.5344 us\n",
      "        12     3       3  0.771357   0.690879 0.982167  0.811165          84000                 5.5480 us\n",
      "        12     4       3  0.778619   0.701981 0.968333  0.813921          84000                 5.5220 us\n",
      "        12     5       3  0.820702   0.747269 0.969190  0.843884          84000                 6.7466 us\n",
      "        13     1       3  0.769071   0.692344 0.968524  0.807472          84000                 5.5262 us\n",
      "        13     2       3  0.764524   0.686122 0.975143  0.805491          84000                 5.5938 us\n",
      "        13     3       3  0.768607   0.691931 0.968357  0.807132          84000                 5.5270 us\n",
      "        13     4       3  0.838679   0.768940 0.968333  0.857194          84000                 6.7600 us\n",
      "        13     5       3  0.748619   0.671704 0.972595  0.794619          84000                 5.5981 us\n",
      "        14     1       3  0.922512   0.937912 0.904929  0.921125          84000                 5.5360 us\n",
      "        14     2       3  0.915524   0.923305 0.906333  0.914740          84000                 5.5233 us\n",
      "        14     3       3  0.908000   0.910600 0.904833  0.907708          84000                 5.5259 us\n",
      "        14     4       3  0.909643   0.915279 0.902857  0.909026          84000                 6.7241 us\n",
      "        14     5       3  0.908571   0.912937 0.903286  0.908086          84000                 5.5782 us\n",
      "        15     1       3  0.986893   0.976645 0.997643  0.987032          84000                 5.5460 us\n",
      "        15     2       3  0.914774   0.923619 0.904333  0.913875          84000                 5.5546 us\n",
      "        15     3       3  0.985869   0.973524 0.998905  0.986051          84000                 6.7571 us\n",
      "        15     4       3  0.915833   0.926913 0.902857  0.914727          84000                 5.5692 us\n",
      "        15     5       3  0.915560   0.926017 0.903286  0.914510          84000                 5.5323 us\n",
      "        16     1       3  0.898548   0.892308 0.906500  0.899348          84000                 5.5453 us\n",
      "        16     2       3  0.917976   0.927920 0.906357  0.917012          84000                 5.5667 us\n",
      "        16     3       3  0.989190   0.979599 0.999190  0.989298          84000                 6.7402 us\n",
      "        16     4       3  0.934500   0.954724 0.912262  0.933010          84000                 5.5764 us\n",
      "        16     5       3  0.909679   0.914984 0.903286  0.909097          84000                 5.5186 us\n",
      "        17     1       3  0.905512   0.905637 0.905357  0.905497          84000                 5.5250 us\n",
      "        17     2       3  0.908845   0.912568 0.904333  0.908432          84000                 5.5273 us\n",
      "        17     3       3  0.935476   0.956087 0.912881  0.933985          84000                 6.7268 us\n",
      "        17     4       3  0.915202   0.925713 0.902857  0.914142          84000                 5.5699 us\n",
      "        17     5       3  0.910714   0.916908 0.903286  0.910046          84000                 5.5160 us\n",
      "        18     1       3  0.976464   0.955086 0.999952  0.977004          84000                 5.5325 us\n",
      "        18     2       3  0.987000   0.975896 0.998667  0.987150          84000                 6.7220 us\n",
      "        18     3       3  0.918595   0.930654 0.904595  0.917439          84000                 5.5458 us\n",
      "        18     4       3  0.986690   0.975151 0.998833  0.986850          84000                 5.5066 us\n",
      "        18     5       3  0.991155   0.982730 0.999881  0.991231          84000                 5.5644 us\n",
      "        19     1       3  0.988702   0.979243 0.998571  0.988813          84000                 5.5637 us\n",
      "        19     2       3  0.934131   0.953739 0.912524  0.932676          84000                 6.7397 us\n",
      "        19     3       3  0.987845   0.976489 0.999762  0.987988          84000                 5.6225 us\n",
      "        19     4       3  0.987202   0.976127 0.998833  0.987350          84000                 5.5061 us\n",
      "        19     5       3  0.990667   0.982578 0.999048  0.990744          84000                 5.5443 us\n",
      "        20     1       3  0.989321   0.980433 0.998571  0.989419          84000                 6.7406 us\n",
      "        20     2       3  0.990619   0.982576 0.998952  0.990697          84000                 5.5238 us\n",
      "        20     3       3  0.992762   0.985958 0.999762  0.992812          84000                 5.5584 us\n",
      "        20     4       3  0.991417   0.983123 1.000000  0.991490          84000                 5.5210 us\n",
      "        20     5       3  0.925512   0.937052 0.912310  0.924515          84000                 5.5550 us\n",
      "        21     1       3  0.921476   0.929514 0.912119  0.920734          84000                 6.7602 us\n",
      "        21     2       3  0.995690   0.991849 0.999595  0.995707          84000                 5.5525 us\n",
      "        21     3       3  0.984167   0.970827 0.998333  0.984388          84000                 5.5668 us\n",
      "        21     4       3  0.941345   0.968602 0.912262  0.939588          84000                 5.5733 us\n",
      "        21     5       3  0.990155   0.981589 0.999048  0.990242          84000                 5.5482 us\n",
      "        22     1       3  0.993357   0.988265 0.998571  0.993392          84000                 6.6792 us\n",
      "        22     2       3  0.996786   0.993612 1.000000  0.996796          84000                 5.5683 us\n",
      "        22     3       3  0.993774   0.988758 0.998905  0.993806          84000                 5.5219 us\n",
      "        22     4       3  0.980679   0.965034 0.997500  0.980998          84000                 5.5352 us\n",
      "        22     5       3  0.992393   0.985014 1.000000  0.992450          84000                 6.7534 us\n",
      "        23     1       3  0.988119   0.977767 0.998952  0.988246          84000                 5.5696 us\n",
      "        23     2       3  0.990476   0.982571 0.998667  0.990554          84000                 5.5354 us\n",
      "        23     3       3  0.933000   0.951154 0.912881  0.931624          84000                 5.5287 us\n",
      "        23     4       3  0.795357   0.709579 1.000000  0.830122          84000                 5.5613 us\n",
      "        23     5       3  0.991060   0.983338 0.999048  0.991130          84000                 6.7205 us\n",
      "        24     1       3  0.928905   0.950463 0.904976  0.927162          84000                 5.5600 us\n",
      "        24     2       3  0.981857   0.964985 1.000000  0.982180          84000                 5.5323 us\n",
      "        24     3       3  0.994440   0.989234 0.999762  0.994470          84000                 5.5841 us\n",
      "        24     4       3  0.987798   0.976177 1.000000  0.987945          84000                 6.7688 us\n",
      "        24     5       3  0.996226   0.992509 1.000000  0.996240          84000                 5.5640 us\n",
      "        12     1       4  0.861250   0.792358 0.979071  0.875875          84000                 5.5461 us\n",
      "        12     2       4  0.499774   0.000000 0.000000  0.000000          84000                 5.5011 us\n",
      "        12     3       4  0.793095   0.713259 0.980261  0.825712          84000                 5.6747 us\n",
      "        12     4       4  0.798012   0.718660 0.979477  0.829039          84000                 6.7721 us\n",
      "        12     5       4  0.780583   0.700686 0.979643  0.817009          84000                 5.5345 us\n",
      "        13     1       4  0.779095   0.699347 0.979119  0.815916          84000                 5.5268 us\n",
      "        13     2       4  0.764095   0.684412 0.980143  0.806007          84000                 5.5772 us\n",
      "        13     3       4  0.773845   0.693822 0.980261  0.812536          84000                 5.5147 us\n",
      "        13     4       4  0.774262   0.692998 0.984810  0.813528          84000                 6.7263 us\n",
      "        13     5       4  0.783119   0.703237 0.979643  0.818741          84000                 5.5489 us\n",
      "        14     1       4  0.930333   0.954509 0.903738  0.928430          84000                 5.5147 us\n",
      "        14     2       4  0.981952   0.969086 0.995667  0.982197          84000                 5.5901 us\n",
      "        14     3       4  0.890631   0.879294 0.905569  0.892238          84000                 6.7672 us\n",
      "        14     4       4  0.981512   0.968604 0.995286  0.981764          84000                 5.6225 us\n",
      "        14     5       4  0.927262   0.948087 0.904024  0.925531          84000                 5.5473 us\n",
      "        15     1       4  0.927381   0.948593 0.903738  0.925622          84000                 5.5074 us\n",
      "        15     2       4  0.988012   0.976585 1.000000  0.988154          84000                 5.6481 us\n",
      "        15     3       4  0.925202   0.942577 0.905569  0.923703          84000                 6.7266 us\n",
      "        15     4       4  0.911345   0.925144 0.895122  0.909885          84000                 5.5359 us\n",
      "        15     5       4  0.986774   0.974251 0.999976  0.986946          84000                 5.5271 us\n",
      "        16     1       4  0.987548   0.978390 0.997119  0.987666          84000                 5.5675 us\n",
      "        16     2       4  0.917286   0.928236 0.904500  0.916214          84000                 6.7922 us\n",
      "        16     3       4  0.922798   0.937969 0.905474  0.921435          84000                 5.5040 us\n",
      "        16     4       4  0.929131   0.951503 0.904359  0.927332          84000                 5.5529 us\n",
      "        16     5       4  0.980798   0.967129 0.995429  0.981075          84000                 5.5123 us\n",
      "        17     1       4  0.984321   0.969618 0.999976  0.984563          84000                 5.5358 us\n",
      "        17     2       4  0.931155   0.955434 0.904500  0.929269          84000                 6.7316 us\n",
      "        17     3       4  0.924964   0.942110 0.905569  0.923478          84000                 5.5971 us\n",
      "        17     4       4  0.989095   0.983114 0.995286  0.989163          84000                 5.5027 us\n",
      "        17     5       4  0.987155   0.974975 0.999976  0.987317          84000                 5.5530 us\n",
      "        18     1       4  0.984131   0.972290 0.996667  0.984327          84000                 5.5255 us\n",
      "        18     2       4  0.988536   0.980831 0.996548  0.988627          84000                 6.8889 us\n",
      "        18     3       4  0.985976   0.972717 1.000000  0.986170          84000                 5.6860 us\n",
      "        18     4       4  0.997321   0.994672 1.000000  0.997329          84000                 5.7321 us\n",
      "        18     5       4  0.933190   0.955784 0.908405  0.931492          84000                 5.6927 us\n",
      "        19     1       4  0.919988   0.929676 0.908714  0.919076          84000                 6.7430 us\n",
      "        19     2       4  0.988155   0.981020 0.995571  0.988242          84000                 5.7285 us\n",
      "        19     3       4  0.993774   0.987700 1.000000  0.993812          84000                 5.7137 us\n",
      "        19     4       4  0.989167   0.983208 0.995333  0.989233          84000                 5.7336 us\n",
      "        19     5       4  0.993988   0.988142 0.999976  0.994024          84000                 5.6013 us\n",
      "        20     1       4  0.992440   0.985129 0.999976  0.992497          84000                 6.4947 us\n",
      "        20     2       4  0.985667   0.975988 0.995833  0.985811          84000                 5.5413 us\n",
      "        20     3       4  0.991798   0.983860 1.000000  0.991864          84000                 5.5627 us\n",
      "        20     4       4  0.925488   0.944265 0.904359  0.923881          84000                 5.5440 us\n",
      "        20     5       4  0.988310   0.977175 0.999976  0.988444          84000                 6.7276 us\n",
      "        21     1       4  0.936083   0.961359 0.908690  0.934283          84000                 5.5401 us\n",
      "        21     2       4  0.934440   0.957799 0.908929  0.932724          84000                 5.5735 us\n",
      "        21     3       4  0.933333   0.958622 0.905760  0.931441          84000                 5.5307 us\n",
      "        21     4       4  0.989476   0.979387 1.000000  0.989586          84000                 5.5475 us\n",
      "        21     5       4  0.932571   0.954541 0.908405  0.930902          84000                 6.7591 us\n",
      "        22     1       4  0.995536   0.991174 0.999976  0.995555          84000                 5.5566 us\n",
      "        22     2       4  0.992964   0.986124 1.000000  0.993013          84000                 5.5996 us\n",
      "        22     3       4  0.990774   0.981882 1.000000  0.990858          84000                10.5886 us\n",
      "        22     4       4  0.932571   0.953906 0.909074  0.930950          84000                10.6769 us\n",
      "        22     5       4  0.994119   0.988398 0.999976  0.994153          84000                 7.1661 us\n",
      "        23     1       4  0.989917   0.980254 0.999976  0.990017          84000                 5.5528 us\n",
      "        23     2       4  0.995893   0.991853 1.000000  0.995910          84000                 5.5322 us\n",
      "        23     3       4  0.996071   0.992204 1.000000  0.996087          84000                 5.5817 us\n",
      "        23     4       4  0.865976   0.985840 0.742625  0.847121          84000                 6.7981 us\n",
      "        23     5       4  0.994143   0.988444 0.999976  0.994177          84000                 5.5433 us\n",
      "        24     1       4  0.995440   0.990987 0.999976  0.995461          84000                 5.5873 us\n",
      "        24     2       4  0.990024   0.980438 1.000000  0.990122          84000                 5.5321 us\n",
      "        24     3       4  0.981048   0.963479 1.000000  0.981400          84000                 5.5814 us\n",
      "        24     4       4  0.996607   0.993260 1.000000  0.996619          84000                 6.5079 us\n",
      "        24     5       4  0.931631   0.952658 0.908405  0.930005          84000                 5.5682 us\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_data = {}\n",
    "result_df, store_results_df = accumulate_results(result_sources, confusion_matrix_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee64a5-4404-4e51-aed9-cbcc52e1d352",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix (per client per Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369edecd-71ed-4395-9f49-15e6138f84fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "860421bd-e59e-4d79-be72-7bc5aa089873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    {\n",
    "        'client_id': 1,\n",
    "        'plot_name': 'Client 1',\n",
    "        'plot_position': [0, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 2,\n",
    "        'plot_name': 'Client 2',\n",
    "        'plot_position': [0, 1]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 3,\n",
    "        'plot_name': 'Client 3',\n",
    "        'plot_position': [1, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 4,\n",
    "        'plot_name': 'Client 4',\n",
    "        'plot_position': [1, 1]\n",
    "    }   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd872bc-f4dd-400d-9546-7751662492bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
