{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424c0338-ba52-45c7-b636-7c05ff16352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from dataloader import get_evaluation_datasets_by_client  # Assuming this function gets local client datasets\n",
    "from model import Net\n",
    "from collections import OrderedDict\n",
    "from config import NUM_CLASSES, MODEL_PATH, BATCH_SIZE\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import to_tensor\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef994e1c-6907-4e54-8e2a-e623db313884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ad0a5-4b91-474f-97ef-a0f36ceacc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de8f59b-1122-4aab-aec3-84239c5baf44",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5a2eaf-1391-4c5e-a4cb-ebc40517757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the global model from the saved path\n",
    "def load_model(input_size, num_classes=NUM_CLASSES, model_path=MODEL_PATH):\n",
    "    model = Net(input_size=input_size, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e4f19d-02cd-42b4-a9ca-97dadfb76ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a client's dataset\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Get the total number of samples from the DataLoader\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    # Start the timer before the loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # End the timer after the loop\n",
    "    end_time = time.time()    \n",
    "    # Calculate the total inference time\n",
    "    total_inference_time = end_time - start_time    \n",
    "    # Calculate average inference time per sample\n",
    "    #inference_time_per_sample =  total_inference_time * 1000\n",
    "    inference_time_per_sample =  total_inference_time * 1000000 / total_samples\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), f'{inference_time_per_sample:.4f} us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106d4a49-75bd-4cc1-b995-0fdc5e26b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional: Set a title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59336a77-e75c-4299-9b8b-d9be081b3510",
   "metadata": {},
   "source": [
    "## 2. Performance/History of Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bf58d5-b6f7-4295-8907-44d74e8bdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sources = {\n",
    "    'components': [12, 14, 16, 18, 20, 22],\n",
    "    'folds': [1, 2, 3, 4, 5],\n",
    "    'marker': ['o', '-', '^' 'x', '-o-'],\n",
    "    'clients': [1, 2, 3, 4],\n",
    "    'path': './results/client_{0}/feature_{1}_fold_{2}_model.pth'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b5d34-d75b-48b9-a7d2-52a86a0cc712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55aeb134-0bf6-4f4f-8018-6cfc65266b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc834c8-24d6-466f-a742-4fbba263fb18",
   "metadata": {},
   "source": [
    "### 2.1 Accuracy/Loss vs Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f53caf3-4641-4fb3-a80e-5e6a70b42745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component in result_sources.get('components'):    \n",
    "#     loss_distributed = [] \n",
    "#     loss_centralized = [] \n",
    "#     accuracy_distributed =[] \n",
    "#     accuracy_centralized = []\n",
    "   \n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         history_path = result_sources.get('path').format(component, fold) + '/history.pkl'        \n",
    "#         l_d, l_c, a_d, a_c = parse_history(history_path)        \n",
    "#         loss_distributed.append(l_d)\n",
    "#         loss_centralized.append(l_c)\n",
    "#         accuracy_distributed.append(a_d)\n",
    "#         accuracy_centralized.append(a_c)\n",
    "\n",
    "#     history_plots = [\n",
    "#         {\n",
    "#             'type': 'distributed_loss',\n",
    "#             'plot_name': 'Distributed Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [0, 0],\n",
    "#             'data': loss_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'accuracy_distributed',\n",
    "#             'plot_name': 'Distributed Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [0, 1],\n",
    "#             'data': accuracy_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#          {\n",
    "#             'type': 'centralized_loss',\n",
    "#             'plot_name': 'Centralized Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [1, 0],\n",
    "#             'data': loss_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'centralized_accuracy',\n",
    "#             'plot_name': 'Centralized Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [1, 1],\n",
    "#             'data': accuracy_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },     \n",
    "      \n",
    "#     ]\n",
    "\n",
    "#     fig, ax = plt.subplots(2, 2, figsize=(16, 9))  # Adjust the figsize as needed\n",
    "#     for plot in  history_plots:\n",
    "#         position = plot.get('plot_position')\n",
    "#         all_fold_data = plot.get('data')\n",
    "#         #print(len(all_fold_data))\n",
    "#         for i, data in enumerate(all_fold_data):\n",
    "#             rounds = list(range(1, len(data)+1))\n",
    "#             ax[position[0], position[1]].plot(rounds, data, label=f'Fold_{i+1}', marker='o', color=plot.get('colors')[i])\n",
    "#             ax[position[0], position[1]].set_title(plot.get('plot_name').format(component))\n",
    "#             ax[position[0], position[1]].set_xlabel(plot.get('x'))\n",
    "#             ax[position[0], position[1]].set_ylabel(plot.get('y'))\n",
    "#             ax[position[0], position[1]].legend()\n",
    "#             ax[position[0], position[1]].grid(True)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331ad99-9689-4909-824a-f4921522fe14",
   "metadata": {},
   "source": [
    "### 2.2 Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2107e6-fb98-4588-8ffa-d206223fe877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##Plotting the Time charts\n",
    "# # for component in result_sources.get('components'):\n",
    "# #     training_time = []\n",
    "# #     for fold in result_sources.get('folds'):\n",
    "# #         ##Parsign the training time\n",
    "# #         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "# #         training_time = parse_training_time(training_time_path)\n",
    "# #         print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "# for component in result_sources.get('components'):\n",
    "#     training_time = []\n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         ##Parsign the training time\n",
    "#         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "#         training_time.append(parse_training_time(training_time_path))\n",
    "#         #print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "#     training_time_to_string = \", \".join(map(str, training_time))\n",
    "#     print(f'{component}, {training_time_to_string}')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b5db2-b662-4581-84fd-87faceaab836",
   "metadata": {},
   "source": [
    "## 3. Accumulate Results\n",
    "- Accumulate all thre results and save in csv file\n",
    "- It also stores values reauired for confusion matrix in a varialbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf6381e-b55e-4af1-ad87-c274d9c818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "client_metrics = {\n",
    "    'Component': [],\n",
    "    'Fold': [],\n",
    "    'Client': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1_Score': [],\n",
    "    'Sample_Number': [],\n",
    "    'Inference_Time_Per_Sample': []\n",
    "}\n",
    "\n",
    "classes = np.arange(NUM_CLASSES)  # Define or import this variable\n",
    "\n",
    "def accumulate_results(results, confusion_matrix_data):\n",
    "    components = results.get('components')\n",
    "    folds = results.get('folds')\n",
    "    path = results.get('path')\n",
    "    clients = results.get('clients')\n",
    "\n",
    "    for client in clients:    \n",
    "        for component in components:  \n",
    "            for fold in folds:\n",
    "                testset = get_evaluation_datasets_by_client(client, fold=fold, feature_count=component)  \n",
    "                testloader = DataLoader(to_tensor(testset), batch_size=BATCH_SIZE)                \n",
    "                \n",
    "                model_path = path.format(client, component, fold)                        \n",
    "                model = load_model(model_path=model_path, input_size=component, num_classes=NUM_CLASSES)\n",
    "                model.to(device)\n",
    "\n",
    "                preds, labels, inference_time_per_sample = run_inference(model, testloader, device)                                  \n",
    "                   \n",
    "                client_metrics['Component'].append(component)\n",
    "                client_metrics['Fold'].append(fold)\n",
    "                client_metrics['Client'].append(client)\n",
    "                client_metrics['Accuracy'].append(accuracy_score(labels, preds))\n",
    "                client_metrics['Precision'].append(precision_score(labels, preds))\n",
    "                client_metrics['Recall'].append(recall_score(labels, preds))\n",
    "                client_metrics['F1_Score'].append(f1_score(labels, preds))\n",
    "                client_metrics['Sample_Number'].append(len(labels)),\n",
    "                client_metrics['Inference_Time_Per_Sample'].append(inference_time_per_sample)\n",
    "\n",
    "                #Saving info for confusion matrix\n",
    "                key = f'{component}_{fold}_{client}'\n",
    "                confusion_matrix_data[key] = {\n",
    "                    'preds': preds,\n",
    "                    'labels': labels,\n",
    "                    'classes': np.arange(NUM_CLASSES)\n",
    "                }   \n",
    "\n",
    "    ##Converting into datafram for better visualization\n",
    "    df = pd.DataFrame(client_metrics)    \n",
    "    return df, confusion_matrix_data           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c6cb12-f99f-4019-9a45-c9c814003fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_data = {}\n",
    "result_df, store_results_df = accumulate_results(result_sources, confusion_matrix_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec77a390-8c97-4699-aea1-8e0204041f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Component  Fold  Client  Accuracy  Precision   Recall  F1_Score  Sample_Number Inference_Time_Per_Sample\n",
      "        12     1       1  0.363636   0.000000 0.000000  0.000000         110000                27.0941 us\n",
      "        12     2       1  0.320009   0.000000 0.000000  0.000000         110000                10.0895 us\n",
      "        12     3       1  0.822309   0.781727 0.999986  0.877488         110000                10.8935 us\n",
      "        12     4       1  0.362073   0.005747 0.000014  0.000029         110000                10.0948 us\n",
      "        12     5       1  0.645836   0.654223 0.940586  0.771695         110000                10.7859 us\n",
      "        14     1       1  0.827455   0.816643 0.939886  0.873941         110000                10.6659 us\n",
      "        14     2       1  0.837773   0.828519 0.939529  0.880539         110000                 9.8907 us\n",
      "        14     3       1  0.872018   0.868937 0.940786  0.903435         110000                10.0271 us\n",
      "        14     4       1  0.842173   0.832632 0.941171  0.883581         110000                10.8876 us\n",
      "        14     5       1  0.872400   0.869555 0.940586  0.903677         110000                10.0568 us\n",
      "        16     1       1  0.875336   0.873776 0.939871  0.905620         110000                 9.8680 us\n",
      "        16     2       1  0.826155   0.815391 0.939529  0.873069         110000                 9.8998 us\n",
      "        16     3       1  0.836645   0.826504 0.940786  0.879950         110000                10.8568 us\n",
      "        16     4       1  0.832000   0.821021 0.941171  0.877000         110000                10.0568 us\n",
      "        16     5       1  0.832736   0.822180 0.940586  0.877406         110000                10.9504 us\n",
      "        18     1       1  0.832955   0.822822 0.939886  0.877467         110000                10.8771 us\n",
      "        18     2       1  0.531209   0.922407 0.287514  0.438384         110000                10.0314 us\n",
      "        18     3       1  0.857827   0.851401 0.940786  0.893864         110000                10.0580 us\n",
      "        18     4       1  0.864736   0.971547 0.811200  0.884162         110000                10.9190 us\n",
      "        18     5       1  0.857155   0.850714 0.940586  0.893395         110000                10.0720 us\n",
      "        20     1       1  0.863836   0.971547 0.809743  0.883296         110000                10.1414 us\n",
      "        20     2       1  0.889918   0.852525 1.000000  0.920393         110000                10.7822 us\n",
      "        20     3       1  0.565100   0.757512 0.465643  0.576755         110000                10.6818 us\n",
      "        20     4       1  0.894891   0.858252 0.999986  0.923713         110000                 9.9802 us\n",
      "        20     5       1  0.862645   0.857405 0.940586  0.897071         110000                10.0538 us\n",
      "        22     1       1  0.941745   0.967657 0.939871  0.953562         110000                10.9209 us\n",
      "        22     2       1  0.852736   0.846063 0.939529  0.890349         110000                10.1009 us\n",
      "        22     3       1  0.857545   0.851378 0.940286  0.893626         110000                10.0348 us\n",
      "        22     4       1  0.923764   0.999044 0.881043  0.936340         110000                10.4304 us\n",
      "        22     5       1  0.901727   0.866229 1.000000  0.928320         110000                10.3103 us\n",
      "        12     1       2  0.873765   0.838012 1.000000  0.911868         107189                 9.4870 us\n",
      "        12     2       2  0.840637   0.803840 1.000000  0.891254         107189                 9.6578 us\n",
      "        12     3       2  0.705781   0.689408 0.999986  0.816148         107189                10.6859 us\n",
      "        12     4       2  0.857820   0.821211 1.000000  0.901829         107188                 9.9910 us\n",
      "        12     5       2  0.660848   0.658185 1.000000  0.793862         107188                10.0083 us\n",
      "        14     1       2  0.960640   0.999802 0.939914  0.968934         107189                10.9439 us\n",
      "        14     2       2  0.961125   0.999924 0.940543  0.969325         107189                10.1199 us\n",
      "        14     3       2  0.961078   0.999848 0.940543  0.969289         107189                10.0913 us\n",
      "        14     4       2  0.960257   0.999165 0.939929  0.968642         107188                10.9588 us\n",
      "        14     5       2  0.961414   0.999939 0.940971  0.969560         107188                10.1157 us\n",
      "        16     1       2  0.960630   0.999787 0.939914  0.968927         107189                10.7098 us\n",
      "        16     2       2  0.961069   0.999833 0.940543  0.969282         107189                 9.5838 us\n",
      "        16     3       2  0.976938   0.965890 1.000000  0.982649         107189                10.6135 us\n",
      "        16     4       2  0.960555   0.999651 0.939929  0.968870         107188                 9.8725 us\n",
      "        16     5       2  0.960350   0.998212 0.940971  0.968747         107188                10.0647 us\n",
      "        18     1       2  0.966452   0.999774 0.948843  0.973643         107189                10.9486 us\n",
      "        18     2       2  0.972674   0.959838 1.000000  0.979507         107189                10.0902 us\n",
      "        18     3       2  0.999841   0.999757 1.000000  0.999879         107189                10.1379 us\n",
      "        18     4       2  0.960695   0.999878 0.939929  0.968977         107188                10.6308 us\n",
      "        18     5       2  0.961414   0.999939 0.940971  0.969560         107188                10.4739 us\n",
      "        20     1       2  0.960612   0.999757 0.939914  0.968912         107189                 9.8809 us\n",
      "        20     2       2  0.999832   0.999743 1.000000  0.999871         107189                 9.9205 us\n",
      "        20     3       2  0.967263   0.999835 0.950029  0.974295         107189                10.9080 us\n",
      "        20     4       2  0.960733   0.999939 0.939929  0.969006         107188                10.3655 us\n",
      "        20     5       2  0.999860   0.999786 1.000000  0.999893         107188                 9.6104 us\n",
      "        22     1       2  0.947420   0.998760 0.920629  0.958104         107189                10.7243 us\n",
      "        22     2       2  0.948157   0.999907 0.920700  0.958670         107189                 9.4829 us\n",
      "        22     3       2  0.947065   0.999736 0.919186  0.957770         107189                 9.7171 us\n",
      "        22     4       2  0.948129   0.999891 0.920671  0.958648         107188                 9.8772 us\n",
      "        22     5       2  0.947130   0.999364 0.919629  0.957840         107188                10.8159 us\n",
      "        12     1       3  0.399682   0.926987 0.061486  0.115322         110000                10.0685 us\n",
      "        12     2       3  0.363309   0.000000 0.000000  0.000000         110000                10.0569 us\n",
      "        12     3       3  0.363545   0.000000 0.000000  0.000000         110000                11.5667 us\n",
      "        12     4       3  0.363636   0.000000 0.000000  0.000000         110000                 9.6309 us\n",
      "        12     5       3  0.354809   0.000000 0.000000  0.000000         110000                 9.7390 us\n",
      "        14     1       3  0.815982   0.838159 0.880929  0.859012         110000                10.7630 us\n",
      "        14     2       3  0.825355   0.798208 0.971043  0.876183         110000                 9.4976 us\n",
      "        14     3       3  0.594282   0.700742 0.632600  0.664930         110000                 9.7025 us\n",
      "        14     4       3  0.821773   0.803332 0.953314  0.871921         110000                 9.9323 us\n",
      "        14     5       3  0.773191   0.778014 0.900529  0.834800         110000                10.8000 us\n",
      "        16     1       3  0.585418   0.693797 0.623843  0.656963         110000                10.0946 us\n",
      "        16     2       3  0.524236   0.620716 0.648843  0.634468         110000                10.0662 us\n",
      "        16     3       3  0.363636   0.000000 0.000000  0.000000         110000                10.9090 us\n",
      "        16     4       3  0.555818   0.652464 0.646200  0.649317         110000                10.0719 us\n",
      "        16     5       3  0.363636   0.000000 0.000000  0.000000         110000                10.0791 us\n",
      "        18     1       3  0.768927   0.761276 0.927843  0.836347         110000                11.2390 us\n",
      "        18     2       3  0.909127   0.970280 0.884286  0.925289         110000                 9.6095 us\n",
      "        18     3       3  0.793891   0.755645 0.999243  0.860537         110000                 9.7996 us\n",
      "        18     4       3  0.814073   0.774261 0.999129  0.872438         110000                12.5632 us\n",
      "        18     5       3  0.600082   0.709261 0.629671  0.667101         110000                10.0870 us\n",
      "        20     1       3  0.788109   0.785774 0.917043  0.846349         110000                 9.9803 us\n",
      "        20     2       3  0.775055   0.772804 0.915729  0.838217         110000                10.0958 us\n",
      "        20     3       3  0.900945   0.936308 0.905971  0.920890         110000                10.9487 us\n",
      "        20     4       3  0.776809   0.769206 0.927586  0.841004         110000                10.1848 us\n",
      "        20     5       3  0.584164   0.685430 0.640486  0.662196         110000                10.2642 us\n",
      "        22     1       3  0.781227   0.768761 0.938514  0.845198         110000                11.2985 us\n",
      "        22     2       3  0.847918   0.855894 0.915086  0.884501         110000                10.7843 us\n",
      "        22     3       3  0.803782   0.795032 0.931914  0.858049         110000                 9.7168 us\n",
      "        22     4       3  0.800636   0.793038 0.929214  0.855743         110000                10.8012 us\n",
      "        22     5       3  0.792373   0.780463 0.937414  0.851769         110000                10.0320 us\n",
      "        12     1       4  0.363555   0.000000 0.000000  0.000000         110000                10.0840 us\n",
      "        12     2       4  0.356282   0.000000 0.000000  0.000000         110000                10.0518 us\n",
      "        12     3       4  0.363636   0.000000 0.000000  0.000000         110000                10.4580 us\n",
      "        12     4       4  0.363636   0.000000 0.000000  0.000000         110000                 9.7693 us\n",
      "        12     5       4  0.625945   0.639788 0.943286  0.762445         110000                 9.9646 us\n",
      "        14     1       4  0.363636   0.000000 0.000000  0.000000         110000                10.8781 us\n",
      "        14     2       4  0.661700   0.997995 0.469329  0.638424         110000                 9.3055 us\n",
      "        14     3       4  0.645473   0.996350 0.444514  0.614758         110000                 9.7441 us\n",
      "        14     4       4  0.645073   0.998198 0.443057  0.613713         110000                10.6236 us\n",
      "        14     5       4  0.575273   0.796878 0.446343  0.572193         110000                 9.9919 us\n",
      "        16     1       4  0.644227   0.997229 0.442157  0.612667         110000                 9.9595 us\n",
      "        16     2       4  0.644336   0.994364 0.443614  0.613520         110000                10.0577 us\n",
      "        16     3       4  0.645991   0.998171 0.444514  0.615105         110000                10.8328 us\n",
      "        16     4       4  0.505664   0.662000 0.456014  0.540032         110000                 9.5542 us\n",
      "        16     5       4  0.771709   0.762804 0.930643  0.838406         110000                 9.6992 us\n",
      "        18     1       4  0.566682   0.781901 0.442500  0.565160         110000                10.4682 us\n",
      "        18     2       4  0.611882   0.822846 0.497129  0.619800         110000                11.5336 us\n",
      "        18     3       4  0.648545   0.990454 0.452071  0.620795         110000                10.1307 us\n",
      "        18     4       4  0.533973   0.714358 0.446014  0.549158         110000                10.9307 us\n",
      "        18     5       4  0.844982   0.861410 0.901429  0.880965         110000                10.0929 us\n",
      "        20     1       4  0.837564   0.852959 0.899871  0.875787         110000                10.0817 us\n",
      "        20     2       4  0.634155   0.967480 0.439886  0.604790         110000                10.9116 us\n",
      "        20     3       4  0.853509   0.873700 0.899886  0.886599         110000                10.1091 us\n",
      "        20     4       4  0.584682   0.820246 0.444843  0.576846         110000                10.1664 us\n",
      "        20     5       4  0.681191   0.986166 0.506114  0.668926         110000                10.2431 us\n",
      "        22     1       4  0.892355   0.992931 0.836800  0.908204         110000                11.1549 us\n",
      "        22     2       4  0.893427   0.993279 0.838200  0.909174         110000                10.5124 us\n",
      "        22     3       4  0.893382   0.993212 0.838186  0.909137         110000                10.5787 us\n",
      "        22     4       4  0.893336   0.992894 0.838386  0.909122         110000                11.5961 us\n",
      "        22     5       4  0.894327   0.993057 0.839814  0.910030         110000                 9.9525 us\n"
     ]
    }
   ],
   "source": [
    "result_df.to_csv(\"./results/results.csv\", index=False)\n",
    "print(result_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee64a5-4404-4e51-aed9-cbcc52e1d352",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix (per client per Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369edecd-71ed-4395-9f49-15e6138f84fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860421bd-e59e-4d79-be72-7bc5aa089873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    {\n",
    "        'client_id': 1,\n",
    "        'plot_name': 'Client 1',\n",
    "        'plot_position': [0, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 2,\n",
    "        'plot_name': 'Client 2',\n",
    "        'plot_position': [0, 1]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 3,\n",
    "        'plot_name': 'Client 3',\n",
    "        'plot_position': [1, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 4,\n",
    "        'plot_name': 'Client 4',\n",
    "        'plot_position': [1, 1]\n",
    "    }   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd872bc-f4dd-400d-9546-7751662492bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
