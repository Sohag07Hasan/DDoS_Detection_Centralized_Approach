{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424c0338-ba52-45c7-b636-7c05ff16352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from dataloader import get_evaluation_datasets_by_client  # Assuming this function gets local client datasets\n",
    "from model import Net\n",
    "from collections import OrderedDict\n",
    "from config import NUM_CLASSES, MODEL_PATH, BATCH_SIZE\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import to_tensor\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef994e1c-6907-4e54-8e2a-e623db313884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ad0a5-4b91-474f-97ef-a0f36ceacc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de8f59b-1122-4aab-aec3-84239c5baf44",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5a2eaf-1391-4c5e-a4cb-ebc40517757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the global model from the saved path\n",
    "def load_model(input_size, num_classes=NUM_CLASSES, model_path=MODEL_PATH):\n",
    "    model = Net(input_size=input_size, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e4f19d-02cd-42b4-a9ca-97dadfb76ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a client's dataset\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Get the total number of samples from the DataLoader\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    # Start the timer before the loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # End the timer after the loop\n",
    "    end_time = time.time()    \n",
    "    # Calculate the total inference time\n",
    "    total_inference_time = end_time - start_time    \n",
    "    # Calculate average inference time per sample\n",
    "    #inference_time_per_sample =  total_inference_time * 1000\n",
    "    inference_time_per_sample =  total_inference_time * 1000000 / total_samples\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), f'{inference_time_per_sample:.4f} us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106d4a49-75bd-4cc1-b995-0fdc5e26b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional: Set a title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59336a77-e75c-4299-9b8b-d9be081b3510",
   "metadata": {},
   "source": [
    "## 2. Performance/History of Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33bf58d5-b6f7-4295-8907-44d74e8bdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sources = {\n",
    "    'components': range(12, 21),\n",
    "    'folds': [1, 2, 3, 4, 5],\n",
    "    'marker': ['o', '-', '^' 'x', '-o-'],\n",
    "    'clients': [1, 2, 3, 4],\n",
    "    'path': './results/client_{0}/feature_{1}_fold_{2}_model.pth'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b5d34-d75b-48b9-a7d2-52a86a0cc712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55aeb134-0bf6-4f4f-8018-6cfc65266b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc834c8-24d6-466f-a742-4fbba263fb18",
   "metadata": {},
   "source": [
    "### 2.1 Accuracy/Loss vs Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f53caf3-4641-4fb3-a80e-5e6a70b42745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component in result_sources.get('components'):    \n",
    "#     loss_distributed = [] \n",
    "#     loss_centralized = [] \n",
    "#     accuracy_distributed =[] \n",
    "#     accuracy_centralized = []\n",
    "   \n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         history_path = result_sources.get('path').format(component, fold) + '/history.pkl'        \n",
    "#         l_d, l_c, a_d, a_c = parse_history(history_path)        \n",
    "#         loss_distributed.append(l_d)\n",
    "#         loss_centralized.append(l_c)\n",
    "#         accuracy_distributed.append(a_d)\n",
    "#         accuracy_centralized.append(a_c)\n",
    "\n",
    "#     history_plots = [\n",
    "#         {\n",
    "#             'type': 'distributed_loss',\n",
    "#             'plot_name': 'Distributed Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [0, 0],\n",
    "#             'data': loss_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'accuracy_distributed',\n",
    "#             'plot_name': 'Distributed Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [0, 1],\n",
    "#             'data': accuracy_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#          {\n",
    "#             'type': 'centralized_loss',\n",
    "#             'plot_name': 'Centralized Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [1, 0],\n",
    "#             'data': loss_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'centralized_accuracy',\n",
    "#             'plot_name': 'Centralized Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [1, 1],\n",
    "#             'data': accuracy_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },     \n",
    "      \n",
    "#     ]\n",
    "\n",
    "#     fig, ax = plt.subplots(2, 2, figsize=(16, 9))  # Adjust the figsize as needed\n",
    "#     for plot in  history_plots:\n",
    "#         position = plot.get('plot_position')\n",
    "#         all_fold_data = plot.get('data')\n",
    "#         #print(len(all_fold_data))\n",
    "#         for i, data in enumerate(all_fold_data):\n",
    "#             rounds = list(range(1, len(data)+1))\n",
    "#             ax[position[0], position[1]].plot(rounds, data, label=f'Fold_{i+1}', marker='o', color=plot.get('colors')[i])\n",
    "#             ax[position[0], position[1]].set_title(plot.get('plot_name').format(component))\n",
    "#             ax[position[0], position[1]].set_xlabel(plot.get('x'))\n",
    "#             ax[position[0], position[1]].set_ylabel(plot.get('y'))\n",
    "#             ax[position[0], position[1]].legend()\n",
    "#             ax[position[0], position[1]].grid(True)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331ad99-9689-4909-824a-f4921522fe14",
   "metadata": {},
   "source": [
    "### 2.2 Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2107e6-fb98-4588-8ffa-d206223fe877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##Plotting the Time charts\n",
    "# # for component in result_sources.get('components'):\n",
    "# #     training_time = []\n",
    "# #     for fold in result_sources.get('folds'):\n",
    "# #         ##Parsign the training time\n",
    "# #         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "# #         training_time = parse_training_time(training_time_path)\n",
    "# #         print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "# for component in result_sources.get('components'):\n",
    "#     training_time = []\n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         ##Parsign the training time\n",
    "#         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "#         training_time.append(parse_training_time(training_time_path))\n",
    "#         #print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "#     training_time_to_string = \", \".join(map(str, training_time))\n",
    "#     print(f'{component}, {training_time_to_string}')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b5db2-b662-4581-84fd-87faceaab836",
   "metadata": {},
   "source": [
    "## 3. Accumulate Results\n",
    "- Accumulate all thre results and save in csv file\n",
    "- It also stores values reauired for confusion matrix in a varialbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf6381e-b55e-4af1-ad87-c274d9c818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "client_metrics = {\n",
    "    'Component': [],\n",
    "    'Fold': [],\n",
    "    'Client': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1_Score': [],\n",
    "    'Sample_Number': [],\n",
    "    'Inference_Time_Per_Sample': []\n",
    "}\n",
    "\n",
    "classes = np.arange(NUM_CLASSES)  # Define or import this variable\n",
    "\n",
    "def accumulate_results(results, confusion_matrix_data):\n",
    "    components = results.get('components')\n",
    "    folds = results.get('folds')\n",
    "    path = results.get('path')\n",
    "    clients = results.get('clients')\n",
    "\n",
    "    for client in clients:    \n",
    "        for component in components:  \n",
    "            for fold in folds:\n",
    "                testset = get_evaluation_datasets_by_client(client, fold=fold, feature_count=component)  \n",
    "                testloader = DataLoader(to_tensor(testset), batch_size=BATCH_SIZE)                \n",
    "                \n",
    "                model_path = path.format(client, component, fold)                        \n",
    "                model = load_model(model_path=model_path, input_size=component, num_classes=NUM_CLASSES)\n",
    "                model.to(device)\n",
    "\n",
    "                preds, labels, inference_time_per_sample = run_inference(model, testloader, device)                                  \n",
    "                   \n",
    "                client_metrics['Component'].append(component)\n",
    "                client_metrics['Fold'].append(fold)\n",
    "                client_metrics['Client'].append(client)\n",
    "                client_metrics['Accuracy'].append(accuracy_score(labels, preds))\n",
    "                client_metrics['Precision'].append(precision_score(labels, preds))\n",
    "                client_metrics['Recall'].append(recall_score(labels, preds))\n",
    "                client_metrics['F1_Score'].append(f1_score(labels, preds))\n",
    "                client_metrics['Sample_Number'].append(len(labels)),\n",
    "                client_metrics['Inference_Time_Per_Sample'].append(inference_time_per_sample)\n",
    "\n",
    "                #Saving info for confusion matrix\n",
    "                key = f'{component}_{fold}_{client}'\n",
    "                confusion_matrix_data[key] = {\n",
    "                    'preds': preds,\n",
    "                    'labels': labels,\n",
    "                    'classes': np.arange(NUM_CLASSES)\n",
    "                }   \n",
    "\n",
    "    ##Converting into datafram for better visualization\n",
    "    df = pd.DataFrame(client_metrics)\n",
    "    print(df.to_string(index=False))\n",
    "    return df, confusion_matrix_data           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c6cb12-f99f-4019-9a45-c9c814003fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Component  Fold  Client  Accuracy  Precision   Recall  F1_Score  Sample_Number Inference_Time_Per_Sample\n",
      "        12     1       1  0.517095   0.517095 1.000000  0.681691          84000                79.6770 us\n",
      "        12     2       1  0.829143   0.787789 0.916452  0.847264          84000                25.8334 us\n",
      "        12     3       1  0.864060   0.791840 0.999977  0.883820          84000                25.9621 us\n",
      "        12     4       1  0.540631   0.529553 1.000000  0.692428          84000                27.0440 us\n",
      "        12     5       1  0.870179   0.816385 0.966273  0.885028          84000                26.1426 us\n",
      "        13     1       1  0.831643   0.790132 0.918340  0.849425          84000                25.8268 us\n",
      "        13     2       1  0.629000   0.582252 1.000000  0.735979          84000                25.4359 us\n",
      "        13     3       1  0.828833   0.786734 0.917762  0.847212          84000                26.1317 us\n",
      "        13     4       1  0.810036   0.731327 1.000000  0.844817          84000                25.0750 us\n",
      "        13     5       1  0.595071   0.560834 0.999977  0.718627          84000                24.9593 us\n",
      "        14     1       1  0.866821   0.806252 0.977300  0.883574          84000                25.2135 us\n",
      "        14     2       1  0.553881   0.541182 0.901879  0.676452          84000                26.2518 us\n",
      "        14     3       1  0.884202   0.818421 0.997329  0.899061          84000                25.4479 us\n",
      "        14     4       1  0.575333   0.554992 0.901876  0.687137          84000                25.5169 us\n",
      "        14     5       1  0.872988   0.815502 0.974952  0.888127          84000                25.5734 us\n",
      "        15     1       1  0.875369   0.805788 1.000000  0.892450          84000                25.8690 us\n",
      "        15     2       1  0.551369   0.539609 0.901879  0.675222          84000                27.0295 us\n",
      "        15     3       1  0.573000   0.553457 0.901853  0.685952          84000                26.1861 us\n",
      "        15     4       1  0.876810   0.807597 1.000000  0.893559          84000                26.3514 us\n",
      "        15     5       1  0.857619   0.797937 0.970394  0.875756          84000                26.3181 us\n",
      "        16     1       1  0.864476   0.807447 0.968989  0.880873          84000                27.4590 us\n",
      "        16     2       1  0.582940   0.560069 0.901879  0.691015          84000                26.4495 us\n",
      "        16     3       1  0.899821   0.842729 0.991251  0.910976          84000                26.6807 us\n",
      "        16     4       1  0.861405   0.803780 0.968367  0.878430          84000                26.4039 us\n",
      "        16     5       1  0.859262   0.800061 0.970325  0.877005          84000                27.1149 us\n",
      "        17     1       1  0.892964   0.828580 0.999862  0.906198          84000                25.5439 us\n",
      "        17     2       1  0.797607   0.718699 1.000000  0.836329          84000                25.7111 us\n",
      "        17     3       1  0.559631   0.544812 0.901853  0.679273          84000                25.8995 us\n",
      "        17     4       1  0.582107   0.559502 0.901876  0.690583          84000                27.0626 us\n",
      "        17     5       1  0.564274   0.547796 0.901858  0.681589          84000                26.1943 us\n",
      "        18     1       1  0.971869   0.948405 1.000000  0.973519          84000                26.3644 us\n",
      "        18     2       1  0.975024   0.953925 1.000000  0.976419          84000                26.4122 us\n",
      "        18     3       1  0.976048   0.955749 0.999977  0.977363          84000                26.0889 us\n",
      "        18     4       1  0.972560   0.949606 1.000000  0.974152          84000                26.8690 us\n",
      "        18     5       1  0.977940   0.959106 0.999977  0.979115          84000                25.5600 us\n",
      "        19     1       1  0.966917   0.939868 1.000000  0.969002          84000                25.4424 us\n",
      "        19     2       1  0.976143   0.955898 1.000000  0.977452          84000                25.4636 us\n",
      "        19     3       1  0.971488   0.947742 1.000000  0.973170          84000                26.6809 us\n",
      "        19     4       1  0.979214   0.961355 1.000000  0.980297          84000                25.7048 us\n",
      "        19     5       1  0.975738   0.955204 0.999977  0.977078          84000                25.8578 us\n",
      "        20     1       1  0.977548   0.958387 1.000000  0.978751          84000                25.9942 us\n",
      "        20     2       1  0.964702   0.936101 1.000000  0.966996          84000                27.0302 us\n",
      "        20     3       1  0.977298   0.957962 0.999977  0.978519          84000                26.1267 us\n",
      "        20     4       1  0.978179   0.959508 1.000000  0.979336          84000                26.1990 us\n",
      "        20     5       1  0.977131   0.957668 0.999977  0.978365          84000                26.4025 us\n",
      "        12     1       2  0.813119   0.791767 0.901841  0.843227          84000                27.5962 us\n",
      "        12     2       2  0.794226   0.768878 0.901839  0.830068          84000                26.6091 us\n",
      "        12     3       2  0.809607   0.787406 0.901839  0.840747          84000                26.6948 us\n",
      "        12     4       2  0.813857   0.792674 0.901861  0.843749          84000                26.6216 us\n",
      "        12     5       2  0.814262   0.759021 0.976844  0.854266          84000                26.4086 us\n",
      "        13     1       2  0.557750   0.557545 1.000000  0.715928          84000                27.3139 us\n",
      "        13     2       2  0.829333   0.775366 0.976715  0.864471          84000                26.1231 us\n",
      "        13     3       2  0.824607   0.770792 0.975284  0.861063          84000                25.9346 us\n",
      "        13     4       2  0.837393   0.784235 0.977014  0.870074          84000                25.8053 us\n",
      "        13     5       2  0.816488   0.795980 0.901863  0.845620          84000                26.9987 us\n",
      "        14     1       2  0.820690   0.801325 0.901841  0.848617          84000                26.0285 us\n",
      "        14     2       2  0.978464   0.962793 1.000000  0.981044          84000                26.0764 us\n",
      "        14     3       2  0.819500   0.799803 0.901839  0.847762          84000                26.2087 us\n",
      "        14     4       2  0.820095   0.800550 0.901861  0.848191          84000                27.3371 us\n",
      "        14     5       2  0.812702   0.791236 0.901863  0.842935          84000                26.4113 us\n",
      "        15     1       2  0.813036   0.791663 0.901841  0.843168          84000                26.4425 us\n",
      "        15     2       2  0.900679   0.918458 0.901839  0.910073          84000                26.4445 us\n",
      "        15     3       2  0.902810   0.922049 0.901839  0.911832          84000                27.5396 us\n",
      "        15     4       2  0.807476   0.784776 0.901861  0.839254          84000                26.3571 us\n",
      "        15     5       2  0.836107   0.821506 0.901863  0.859811          84000                26.2099 us\n",
      "        16     1       2  0.819464   0.799761 0.901841  0.847739          84000                 9.6405 us\n",
      "        16     2       2  0.775798   0.747795 0.901839  0.817625          84000                 9.6679 us\n",
      "        16     3       2  0.826690   0.809061 0.901839  0.852935          84000                10.9264 us\n",
      "        16     4       2  0.958405   0.930544 1.000000  0.964022          84000                10.0265 us\n",
      "        16     5       2  0.959821   0.932752 1.000000  0.965206          84000                 9.9866 us\n",
      "        17     1       2  0.605881   0.585750 1.000000  0.738767          84000                10.0407 us\n",
      "        17     2       2  0.839940   0.826695 0.901839  0.862634          84000                11.1118 us\n",
      "        17     3       2  0.814988   0.794101 0.901839  0.844548          84000                10.0872 us\n",
      "        17     4       2  0.828060   0.810836 0.901861  0.853930          84000                10.0517 us\n",
      "        17     5       2  0.817631   0.797424 0.901863  0.846434          84000                11.3105 us\n",
      "        18     1       2  0.968512   0.946519 1.000000  0.972525          84000                11.1197 us\n",
      "        18     2       2  0.970940   0.950439 1.000000  0.974590          84000                10.1374 us\n",
      "        18     3       2  0.971012   0.950554 1.000000  0.974650          84000                10.1238 us\n",
      "        18     4       2  0.557286   0.557280 1.000000  0.715710          84000                10.1363 us\n",
      "        18     5       2  0.970155   0.949168 1.000000  0.973921          84000                11.1901 us\n",
      "        19     1       2  0.972286   0.952625 1.000000  0.975738          84000                10.1753 us\n",
      "        19     2       2  0.972917   0.953653 1.000000  0.976277          84000                10.1765 us\n",
      "        19     3       2  0.971071   0.950651 1.000000  0.974701          84000                10.3864 us\n",
      "        19     4       2  0.971940   0.952062 1.000000  0.975443          84000                10.3724 us\n",
      "        19     5       2  0.971000   0.950536 1.000000  0.974641          84000                11.5854 us\n",
      "        20     1       2  0.970048   0.948994 1.000000  0.973830          84000                10.3517 us\n",
      "        20     2       2  0.973905   0.955268 1.000000  0.977122          84000                10.7027 us\n",
      "        20     3       2  0.970988   0.950516 1.000000  0.974630          84000                11.1051 us\n",
      "        20     4       2  0.973095   0.953944 1.000000  0.976429          84000                12.3318 us\n",
      "        20     5       2  0.969667   0.948379 1.000000  0.973506          84000                11.6069 us\n",
      "        12     1       3  0.708476   0.936317 0.447381  0.605465          84000                11.7910 us\n",
      "        12     2       3  0.730440   0.972791 0.474143  0.637544          84000                12.5728 us\n",
      "        12     3       3  0.669024   0.815287 0.437071  0.569068          84000                11.2072 us\n",
      "        12     4       3  0.660310   0.784309 0.442238  0.565574          84000                10.0945 us\n",
      "        12     5       3  0.659750   0.779691 0.445333  0.566882          84000                10.1240 us\n",
      "        13     1       3  0.691524   0.621901 0.977095  0.760047          84000                10.1695 us\n",
      "        13     2       3  0.639369   0.730244 0.442024  0.550702          84000                11.1583 us\n",
      "        13     3       3  0.694881   0.903083 0.436619  0.588643          84000                10.0585 us\n",
      "        13     4       3  0.679738   0.836079 0.447143  0.582669          84000                10.2555 us\n",
      "        13     5       3  0.808333   0.745759 0.935643  0.829979          84000                10.0677 us\n",
      "        14     1       3  0.768071   0.701500 0.933262  0.800952          84000                10.1320 us\n",
      "        14     2       3  0.806714   0.739647 0.946643  0.830441          84000                11.2432 us\n",
      "        14     3       3  0.773821   0.709386 0.927690  0.803982          84000                10.2930 us\n",
      "        14     4       3  0.605583   0.657168 0.441476  0.528149          84000                10.2901 us\n",
      "        14     5       3  0.783440   0.720026 0.927548  0.810717          84000                10.4973 us\n",
      "        15     1       3  0.693143   0.898507 0.435476  0.586632          84000                11.5853 us\n",
      "        15     2       3  0.691155   0.890662 0.435810  0.585250          84000                10.7133 us\n",
      "        15     3       3  0.712964   0.994089 0.428476  0.598839          84000                10.7510 us\n",
      "        15     4       3  0.708333   0.952103 0.438738  0.600678          84000                10.9768 us\n",
      "        15     5       3  0.701583   0.937523 0.431952  0.591417          84000                12.4439 us\n",
      "        16     1       3  0.683631   0.855923 0.441595  0.582607          84000                11.4628 us\n",
      "        16     2       3  0.697381   0.889092 0.451024  0.598458          84000                11.6821 us\n",
      "        16     3       3  0.781000   0.717308 0.927548  0.808992          84000                11.9525 us\n",
      "        16     4       3  0.608381   0.658950 0.449310  0.534302          84000                13.4404 us\n",
      "        16     5       3  0.694726   0.888509 0.445333  0.593297          84000                12.8092 us\n",
      "        17     1       3  0.620571   0.686629 0.443595  0.538982          84000                13.2219 us\n",
      "        17     2       3  0.782179   0.718344 0.928357  0.809959          84000                13.2450 us\n",
      "        17     3       3  0.782893   0.719414 0.927548  0.810330          84000                13.4464 us\n",
      "        17     4       3  0.606714   0.658531 0.443286  0.529884          84000                15.2183 us\n",
      "        17     5       3  0.651060   0.768892 0.431952  0.553152          84000                15.0534 us\n",
      "        18     1       3  0.701274   0.937937 0.431071  0.590673          84000                13.2127 us\n",
      "        18     2       3  0.584905   0.618566 0.442952  0.516233          84000                 9.9053 us\n",
      "        18     3       3  0.705083   0.957265 0.429333  0.592797          84000                11.0434 us\n",
      "        18     4       3  0.797774   0.728158 0.950333  0.824542          84000                11.1185 us\n",
      "        18     5       3  0.700810   0.914000 0.443333  0.597063          84000                 9.8732 us\n",
      "        19     1       3  0.701655   0.942064 0.429738  0.590232          84000                10.0088 us\n",
      "        19     2       3  0.705893   0.958924 0.430214  0.593955          84000                11.0868 us\n",
      "        19     3       3  0.783440   0.720026 0.927548  0.810717          84000                 9.8352 us\n",
      "        19     4       3  0.782940   0.719152 0.928476  0.810517          84000                 9.9846 us\n",
      "        19     5       3  0.777988   0.714690 0.925405  0.806512          84000                 9.8442 us\n",
      "        20     1       3  0.720167   0.654147 0.934310  0.769522          84000                10.9699 us\n",
      "        20     2       3  0.608060   0.660877 0.443905  0.531085          84000                10.0092 us\n",
      "        20     3       3  0.790881   0.729885 0.923548  0.815375          84000                11.1434 us\n",
      "        20     4       3  0.706167   0.938164 0.441429  0.600369          84000                 9.8814 us\n",
      "        20     5       3  0.701190   0.922500 0.439286  0.595161          84000                10.0745 us\n",
      "        12     1       4  0.780512   0.704707 0.965667  0.814802          84000                12.0644 us\n",
      "        12     2       4  0.751738   0.672801 0.980143  0.797899          84000                 9.8045 us\n",
      "        12     3       4  0.777143   0.697076 0.980261  0.814764          84000                10.8778 us\n",
      "        12     4       4  0.631917   0.695436 0.469441  0.560516          84000                10.0250 us\n",
      "        12     5       4  0.836190   0.761230 0.979667  0.856744          84000                11.1768 us\n",
      "        13     1       4  0.704631   0.920207 0.448119  0.602725          84000                11.1518 us\n",
      "        13     2       4  0.759702   0.694301 0.928000  0.794318          84000                 9.9460 us\n",
      "        13     3       4  0.723036   0.944861 0.473702  0.631036          84000                10.2048 us\n",
      "        13     4       4  0.834024   0.766888 0.959810  0.852572          84000                11.1231 us\n",
      "        13     5       4  0.683964   0.833312 0.459929  0.592719          84000                10.8180 us\n",
      "        14     1       4  0.673476   0.811901 0.451571  0.580355          84000                10.0948 us\n",
      "        14     2       4  0.708119   0.929787 0.450238  0.606693          84000                10.0850 us\n",
      "        14     3       4  0.791286   0.728942 0.927427  0.816292          84000                10.7912 us\n",
      "        14     4       4  0.663143   0.779249 0.455275  0.574752          84000                 9.9091 us\n",
      "        14     5       4  0.725310   0.997529 0.451738  0.621862          84000                 9.9565 us\n",
      "        15     1       4  0.671000   0.804735 0.451571  0.578514          84000                10.2467 us\n",
      "        15     2       4  0.635214   0.708297 0.459786  0.557606          84000                10.1366 us\n",
      "        15     3       4  0.710500   0.936159 0.451796  0.609462          84000                11.5739 us\n",
      "        15     4       4  0.530667   0.531184 0.522559  0.526836          84000                10.1111 us\n",
      "        15     5       4  0.801214   0.740212 0.928190  0.823611          84000                10.0874 us\n",
      "        16     1       4  0.632619   0.704044 0.457595  0.554677          84000                10.1182 us\n",
      "        16     2       4  0.632310   0.705617 0.454048  0.552545          84000                11.0563 us\n",
      "        16     3       4  0.710250   0.935190 0.451796  0.609257          84000                10.0819 us\n",
      "        16     4       4  0.659417   0.769413 0.455299  0.572074          84000                11.7733 us\n",
      "        16     5       4  0.528476   0.532875 0.461571  0.494667          84000                10.0661 us\n",
      "        17     1       4  0.636762   0.717214 0.451571  0.554205          84000                11.1463 us\n",
      "        17     2       4  0.712476   0.946826 0.450238  0.610276          84000                10.1420 us\n",
      "        17     3       4  0.649214   0.743634 0.455416  0.564885          84000                10.1132 us\n",
      "        17     4       4  0.695929   0.883070 0.451680  0.597662          84000                10.1298 us\n",
      "        17     5       4  0.519607   0.522178 0.461643  0.490048          84000                11.1899 us\n",
      "        18     1       4  0.715298   0.962366 0.448119  0.611498          84000                10.1277 us\n",
      "        18     2       4  0.682869   0.830188 0.459786  0.591808          84000                10.1455 us\n",
      "        18     3       4  0.818179   0.747692 0.960451  0.840822          84000                10.2966 us\n",
      "        18     4       4  0.793226   0.729785 0.931287  0.818314          84000                11.6689 us\n",
      "        18     5       4  0.682631   0.834796 0.455381  0.589299          84000                11.0376 us\n",
      "        19     1       4  0.806952   0.743245 0.937905  0.829305          84000                10.1129 us\n",
      "        19     2       4  0.799452   0.734646 0.937548  0.823787          84000                10.0711 us\n",
      "        19     3       4  0.800595   0.733178 0.945142  0.825775          84000                10.0674 us\n",
      "        19     4       4  0.710762   0.931009 0.455275  0.611513          84000                11.1526 us\n",
      "        19     5       4  0.721750   0.980846 0.452333  0.619140          84000                10.1474 us\n",
      "        20     1       4  0.689690   0.862169 0.451571  0.592706          84000                10.0486 us\n",
      "        20     2       4  0.675571   0.819608 0.450238  0.581202          84000                10.1001 us\n",
      "        20     3       4  0.679488   0.825222 0.455416  0.586925          84000                11.3231 us\n",
      "        20     4       4  0.673595   0.808165 0.455275  0.582437          84000                10.1677 us\n",
      "        20     5       4  0.786619   0.723381 0.928167  0.813077          84000                10.9843 us\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_data = {}\n",
    "result_df, store_results_df = accumulate_results(result_sources, confusion_matrix_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee64a5-4404-4e51-aed9-cbcc52e1d352",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix (per client per Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369edecd-71ed-4395-9f49-15e6138f84fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "860421bd-e59e-4d79-be72-7bc5aa089873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    {\n",
    "        'client_id': 1,\n",
    "        'plot_name': 'Client 1',\n",
    "        'plot_position': [0, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 2,\n",
    "        'plot_name': 'Client 2',\n",
    "        'plot_position': [0, 1]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 3,\n",
    "        'plot_name': 'Client 3',\n",
    "        'plot_position': [1, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 4,\n",
    "        'plot_name': 'Client 4',\n",
    "        'plot_position': [1, 1]\n",
    "    }   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd872bc-f4dd-400d-9546-7751662492bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
