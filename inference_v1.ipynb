{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424c0338-ba52-45c7-b636-7c05ff16352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from dataloader import get_evaluation_datasets_by_client  # Assuming this function gets local client datasets\n",
    "from model import Net\n",
    "from collections import OrderedDict\n",
    "from config import NUM_CLASSES, MODEL_PATH, BATCH_SIZE\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import to_tensor\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef994e1c-6907-4e54-8e2a-e623db313884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ad0a5-4b91-474f-97ef-a0f36ceacc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de8f59b-1122-4aab-aec3-84239c5baf44",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5a2eaf-1391-4c5e-a4cb-ebc40517757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the global model from the saved path\n",
    "def load_model(input_size, num_classes=NUM_CLASSES, model_path=MODEL_PATH):\n",
    "    model = Net(input_size=input_size, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e4f19d-02cd-42b4-a9ca-97dadfb76ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a client's dataset\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Get the total number of samples from the DataLoader\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    # Start the timer before the loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # End the timer after the loop\n",
    "    end_time = time.time()    \n",
    "    # Calculate the total inference time\n",
    "    total_inference_time = end_time - start_time    \n",
    "    # Calculate average inference time per sample\n",
    "    #inference_time_per_sample =  total_inference_time * 1000\n",
    "    inference_time_per_sample =  total_inference_time * 1000000 / total_samples\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), f'{inference_time_per_sample:.4f} us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106d4a49-75bd-4cc1-b995-0fdc5e26b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional: Set a title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59336a77-e75c-4299-9b8b-d9be081b3510",
   "metadata": {},
   "source": [
    "## 2. Performance/History of Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bf58d5-b6f7-4295-8907-44d74e8bdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sources = {\n",
    "    'components': range(4, 11),\n",
    "    'folds': [1, 2, 3, 4, 5],\n",
    "    'marker': ['o', '-', '^' 'x', '-o-'],\n",
    "    'clients': [1, 2, 3, 4],\n",
    "    'path': './results/client_{0}/feature_{1}_fold_{2}_model.pth'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b5d34-d75b-48b9-a7d2-52a86a0cc712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55aeb134-0bf6-4f4f-8018-6cfc65266b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc834c8-24d6-466f-a742-4fbba263fb18",
   "metadata": {},
   "source": [
    "### 2.1 Accuracy/Loss vs Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f53caf3-4641-4fb3-a80e-5e6a70b42745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component in result_sources.get('components'):    \n",
    "#     loss_distributed = [] \n",
    "#     loss_centralized = [] \n",
    "#     accuracy_distributed =[] \n",
    "#     accuracy_centralized = []\n",
    "   \n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         history_path = result_sources.get('path').format(component, fold) + '/history.pkl'        \n",
    "#         l_d, l_c, a_d, a_c = parse_history(history_path)        \n",
    "#         loss_distributed.append(l_d)\n",
    "#         loss_centralized.append(l_c)\n",
    "#         accuracy_distributed.append(a_d)\n",
    "#         accuracy_centralized.append(a_c)\n",
    "\n",
    "#     history_plots = [\n",
    "#         {\n",
    "#             'type': 'distributed_loss',\n",
    "#             'plot_name': 'Distributed Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [0, 0],\n",
    "#             'data': loss_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'accuracy_distributed',\n",
    "#             'plot_name': 'Distributed Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [0, 1],\n",
    "#             'data': accuracy_distributed,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#          {\n",
    "#             'type': 'centralized_loss',\n",
    "#             'plot_name': 'Centralized Loss {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Loss',\n",
    "#             'plot_position': [1, 0],\n",
    "#             'data': loss_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'centralized_accuracy',\n",
    "#             'plot_name': 'Centralized Accuracy {}',\n",
    "#             'x': 'Rounds',\n",
    "#             'y': 'Accuracy',\n",
    "#             'plot_position': [1, 1],\n",
    "#             'data': accuracy_centralized,\n",
    "#             'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "#         },     \n",
    "      \n",
    "#     ]\n",
    "\n",
    "#     fig, ax = plt.subplots(2, 2, figsize=(16, 9))  # Adjust the figsize as needed\n",
    "#     for plot in  history_plots:\n",
    "#         position = plot.get('plot_position')\n",
    "#         all_fold_data = plot.get('data')\n",
    "#         #print(len(all_fold_data))\n",
    "#         for i, data in enumerate(all_fold_data):\n",
    "#             rounds = list(range(1, len(data)+1))\n",
    "#             ax[position[0], position[1]].plot(rounds, data, label=f'Fold_{i+1}', marker='o', color=plot.get('colors')[i])\n",
    "#             ax[position[0], position[1]].set_title(plot.get('plot_name').format(component))\n",
    "#             ax[position[0], position[1]].set_xlabel(plot.get('x'))\n",
    "#             ax[position[0], position[1]].set_ylabel(plot.get('y'))\n",
    "#             ax[position[0], position[1]].legend()\n",
    "#             ax[position[0], position[1]].grid(True)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331ad99-9689-4909-824a-f4921522fe14",
   "metadata": {},
   "source": [
    "### 2.2 Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2107e6-fb98-4588-8ffa-d206223fe877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ##Plotting the Time charts\n",
    "# # for component in result_sources.get('components'):\n",
    "# #     training_time = []\n",
    "# #     for fold in result_sources.get('folds'):\n",
    "# #         ##Parsign the training time\n",
    "# #         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "# #         training_time = parse_training_time(training_time_path)\n",
    "# #         print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "# for component in result_sources.get('components'):\n",
    "#     training_time = []\n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         ##Parsign the training time\n",
    "#         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "#         training_time.append(parse_training_time(training_time_path))\n",
    "#         #print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "#     training_time_to_string = \", \".join(map(str, training_time))\n",
    "#     print(f'{component}, {training_time_to_string}')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b5db2-b662-4581-84fd-87faceaab836",
   "metadata": {},
   "source": [
    "## 3. Accumulate Results\n",
    "- Accumulate all thre results and save in csv file\n",
    "- It also stores values reauired for confusion matrix in a varialbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf6381e-b55e-4af1-ad87-c274d9c818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "client_metrics = {\n",
    "    'Component': [],\n",
    "    'Fold': [],\n",
    "    'Client': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1_Score': [],\n",
    "    'Sample_Number': [],\n",
    "    'Inference_Time_Per_Sample': []\n",
    "}\n",
    "\n",
    "classes = np.arange(NUM_CLASSES)  # Define or import this variable\n",
    "\n",
    "def accumulate_results(results, confusion_matrix_data):\n",
    "    components = results.get('components')\n",
    "    folds = results.get('folds')\n",
    "    path = results.get('path')\n",
    "    clients = results.get('clients')\n",
    "\n",
    "    for client in clients:    \n",
    "        for component in components:  \n",
    "            for fold in folds:\n",
    "                testset = get_evaluation_datasets_by_client(client, fold=fold, feature_count=component)  \n",
    "                testloader = DataLoader(to_tensor(testset), batch_size=BATCH_SIZE)                \n",
    "                \n",
    "                model_path = path.format(client, component, fold)                        \n",
    "                model = load_model(model_path=model_path, input_size=component, num_classes=NUM_CLASSES)\n",
    "                model.to(device)\n",
    "\n",
    "                preds, labels, inference_time_per_sample = run_inference(model, testloader, device)                                  \n",
    "                   \n",
    "                client_metrics['Component'].append(component)\n",
    "                client_metrics['Fold'].append(fold)\n",
    "                client_metrics['Client'].append(client)\n",
    "                client_metrics['Accuracy'].append(accuracy_score(labels, preds))\n",
    "                client_metrics['Precision'].append(precision_score(labels, preds))\n",
    "                client_metrics['Recall'].append(recall_score(labels, preds))\n",
    "                client_metrics['F1_Score'].append(f1_score(labels, preds))\n",
    "                client_metrics['Sample_Number'].append(len(labels)),\n",
    "                client_metrics['Inference_Time_Per_Sample'].append(inference_time_per_sample)\n",
    "\n",
    "                #Saving info for confusion matrix\n",
    "                key = f'{component}_{fold}_{client}'\n",
    "                confusion_matrix_data[key] = {\n",
    "                    'preds': preds,\n",
    "                    'labels': labels,\n",
    "                    'classes': np.arange(NUM_CLASSES)\n",
    "                }   \n",
    "\n",
    "    ##Converting into datafram for better visualization\n",
    "    df = pd.DataFrame(client_metrics)\n",
    "    print(df.to_string(index=False))\n",
    "    return df, confusion_matrix_data           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c6cb12-f99f-4019-9a45-c9c814003fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Component  Fold  Client  Accuracy  Precision   Recall  F1_Score  Sample_Number Inference_Time_Per_Sample\n",
      "         4     1       1  0.826691   0.970793 0.750229  0.846377         110000                24.7850 us\n",
      "         4     2       1  0.973782   0.960430 1.000000  0.979816         110000                 9.0757 us\n",
      "         4     3       1  0.825764   0.967276 0.751629  0.845925         110000                 9.5446 us\n",
      "         4     4       1  0.973782   0.960430 1.000000  0.979816         110000                 8.8145 us\n",
      "         4     5       1  0.974600   0.961618 1.000000  0.980433         110000                 9.3035 us\n",
      "         5     1       1  0.974273   0.961142 1.000000  0.980186         110000                10.0626 us\n",
      "         5     2       1  0.826564   0.970733 0.750071  0.846254         110000                 8.6894 us\n",
      "         5     3       1  0.827364   0.970416 0.751629  0.847124         110000                 8.9312 us\n",
      "         5     4       1  0.827018   0.970412 0.751071  0.846768         110000                 9.2114 us\n",
      "         5     5       1  0.827064   0.969462 0.751929  0.846950         110000                 9.6463 us\n",
      "         6     1       1  0.976300   0.964094 1.000000  0.981719         110000                 8.7273 us\n",
      "         6     2       1  0.826564   0.970733 0.750071  0.846254         110000                 8.8284 us\n",
      "         6     3       1  0.976082   0.963776 1.000000  0.981554         110000                 9.7654 us\n",
      "         6     4       1  0.982236   0.972844 1.000000  0.986235         110000                 9.1567 us\n",
      "         6     5       1  0.982227   0.972830 1.000000  0.986228         110000                 8.7435 us\n",
      "         7     1       1  0.982455   0.973168 1.000000  0.986402         110000                 9.4785 us\n",
      "         7     2       1  0.974909   0.962067 1.000000  0.980667         110000                 9.0784 us\n",
      "         7     3       1  0.827282   0.970255 0.751629  0.847062         110000                 9.1241 us\n",
      "         7     4       1  0.982236   0.972844 1.000000  0.986235         110000                 9.7692 us\n",
      "         7     5       1  0.976391   0.964227 1.000000  0.981788         110000                 8.7005 us\n",
      "         8     1       1  0.826682   0.970775 0.750229  0.846370         110000                 9.3838 us\n",
      "         8     2       1  0.900227   0.969533 0.870571  0.917391         110000                 9.2157 us\n",
      "         8     3       1  0.827364   0.970416 0.751629  0.847124         110000                10.2323 us\n",
      "         8     4       1  0.899518   0.968913 0.870014  0.916804         110000                 8.9235 us\n",
      "         8     5       1  0.899473   0.968895 0.869957  0.916765         110000                 9.2289 us\n",
      "         9     1       1  0.824918   0.967306 0.750229  0.845049         110000                 8.8414 us\n",
      "         9     2       1  0.840518   0.999087 0.750071  0.856854         110000                 7.8727 us\n",
      "         9     3       1  0.980673   0.970524 1.000000  0.985041         110000                 7.9528 us\n",
      "         9     4       1  0.899527   0.968928 0.870014  0.916811         110000                 9.1641 us\n",
      "         9     5       1  0.899464   0.968880 0.869957  0.916758         110000                 7.7265 us\n",
      "        10     1       1  0.897736   0.966063 0.869857  0.915439         110000                 7.8774 us\n",
      "        10     2       1  0.899582   0.968439 0.870571  0.916901         110000                 9.0262 us\n",
      "        10     3       1  0.899355   0.968099 0.870529  0.916725         110000                 8.3264 us\n",
      "        10     4       1  0.899564   0.968990 0.870014  0.916839         110000                 7.8732 us\n",
      "        10     5       1  0.981482   0.971723 1.000000  0.985659         110000                 7.7709 us\n",
      "         4     1       2  0.862234   0.972764 0.811771  0.885006         107189                 7.1361 us\n",
      "         4     2       2  0.875640   0.998399 0.810871  0.894917         107189                 6.9856 us\n",
      "         4     3       2  0.876256   0.999577 0.810857  0.895381         107189                 5.8778 us\n",
      "         4     4       2  0.983114   0.974795 1.000000  0.987236         107188                 7.1733 us\n",
      "         4     5       2  0.983776   0.975759 1.000000  0.987731         107188                 5.9450 us\n",
      "         5     1       2  0.982843   0.974401 1.000000  0.987035         107189                 5.9212 us\n",
      "         5     2       2  0.983590   0.975487 1.000000  0.987592         107189                 7.1451 us\n",
      "         5     3       2  0.862831   0.974872 0.810857  0.885333         107189                 6.0125 us\n",
      "         5     4       2  0.874184   0.998571 0.808500  0.893539         107188                 6.1112 us\n",
      "         5     5       2  0.983263   0.975012 1.000000  0.987348         107188                 7.2432 us\n",
      "         6     1       2  0.876377   0.998682 0.811771  0.895578         107189                 6.0298 us\n",
      "         6     2       2  0.983636   0.975555 1.000000  0.987626         107189                 6.0381 us\n",
      "         6     3       2  0.983608   0.975515 1.000000  0.987606         107189                 6.4123 us\n",
      "         6     4       2  0.983151   0.974849 1.000000  0.987264         107188                 7.2940 us\n",
      "         6     5       2  0.977059   0.966064 1.000000  0.982739         107188                 5.4083 us\n",
      "         7     1       2  0.982825   0.974374 1.000000  0.987021         107189                 5.4507 us\n",
      "         7     2       2  0.984056   0.976168 1.000000  0.987940         107189                 7.1044 us\n",
      "         7     3       2  0.862654   0.974554 0.810857  0.885201         107189                 6.5799 us\n",
      "         7     4       2  0.983151   0.974849 1.000000  0.987264         107188                 9.5687 us\n",
      "         7     5       2  0.982965   0.974577 1.000000  0.987125         107188                10.2034 us\n",
      "         8     1       2  0.899141   0.970718 0.871857  0.918635         107189                 8.8512 us\n",
      "         8     2       2  0.875873   0.998839 0.810871  0.895093         107189                 8.7918 us\n",
      "         8     3       2  0.914861   0.999229 0.870300  0.930319         107189                 9.6141 us\n",
      "         8     4       2  0.979727   0.969892 1.000000  0.984716         107188                10.2921 us\n",
      "         8     5       2  0.999403   0.999087 1.000000  0.999543         107188                 9.0666 us\n",
      "         9     1       2  0.982825   0.974374 1.000000  0.987021         107189                 8.8966 us\n",
      "         9     2       2  0.983627   0.975542 1.000000  0.987619         107189                10.4199 us\n",
      "         9     3       2  0.983608   0.975515 1.000000  0.987606         107189                 9.3203 us\n",
      "         9     4       2  0.976490   0.965251 1.000000  0.982318         107188                 8.7847 us\n",
      "         9     5       2  0.999412   0.999101 1.000000  0.999550         107188                 9.7587 us\n",
      "        10     1       2  0.948082   0.999860 0.920629  0.958610         107189                 9.6375 us\n",
      "        10     2       2  0.898973   0.972046 0.870329  0.918379         107189                 9.0300 us\n",
      "        10     3       2  0.976639   0.965464 1.000000  0.982429         107189                 8.8891 us\n",
      "        10     4       2  0.976406   0.965131 1.000000  0.982256         107188                 9.6894 us\n",
      "        10     5       2  0.983776   0.975759 1.000000  0.987731         107188                 9.6104 us\n",
      "         4     1       3  0.999100   0.998588 1.000000  0.999293         110000                 9.0455 us\n",
      "         4     2       3  0.998736   0.998018 1.000000  0.999008         110000                 9.4480 us\n",
      "         4     3       3  0.998636   0.997862 1.000000  0.998930         110000                 8.7769 us\n",
      "         4     4       3  0.998891   0.998260 1.000000  0.999129         110000                 9.5609 us\n",
      "         4     5       3  0.999000   0.998431 1.000000  0.999215         110000                 9.6431 us\n",
      "         5     1       3  0.999064   0.998531 1.000000  0.999265         110000                 8.5629 us\n",
      "         5     2       3  0.999000   0.998431 1.000000  0.999215         110000                 8.7799 us\n",
      "         5     3       3  0.998927   0.998317 1.000000  0.999158         110000                10.2181 us\n",
      "         5     4       3  0.998873   0.998232 1.000000  0.999115         110000                 9.1024 us\n",
      "         5     5       3  0.998873   0.998232 1.000000  0.999115         110000                 8.5737 us\n",
      "         6     1       3  0.999100   0.998588 1.000000  0.999293         110000                 8.7306 us\n",
      "         6     2       3  0.998964   0.998374 1.000000  0.999186         110000                 9.8409 us\n",
      "         6     3       3  0.998927   0.998317 1.000000  0.999158         110000                 9.0539 us\n",
      "         6     4       3  0.998855   0.998203 1.000000  0.999101         110000                 8.5702 us\n",
      "         6     5       3  0.998882   0.998246 1.000000  0.999122         110000                 9.4078 us\n",
      "         7     1       3  0.999064   0.998531 1.000000  0.999265         110000                 9.1217 us\n",
      "         7     2       3  0.999100   0.998588 1.000000  0.999293         110000                 8.9508 us\n",
      "         7     3       3  0.999073   0.998545 1.000000  0.999272         110000                 9.4196 us\n",
      "         7     4       3  0.993255   0.989511 1.000000  0.994728         110000                 9.0877 us\n",
      "         7     5       3  0.998827   0.998161 1.000000  0.999079         110000                 9.2604 us\n",
      "         8     1       3  0.993700   0.990197 1.000000  0.995074         110000                10.1953 us\n",
      "         8     2       3  0.999291   0.998887 1.000000  0.999443         110000                 9.1542 us\n",
      "         8     3       3  0.999300   0.998901 1.000000  0.999450         110000                 8.9613 us\n",
      "         8     4       3  0.993664   0.990141 1.000000  0.995046         110000                 9.2947 us\n",
      "         8     5       3  0.999291   0.998887 1.000000  0.999443         110000                10.1111 us\n",
      "         9     1       3  0.999327   0.998944 1.000000  0.999472         110000                 8.8984 us\n",
      "         9     2       3  0.992945   0.989036 1.000000  0.994488         110000                 8.9703 us\n",
      "         9     3       3  0.999227   0.998787 1.000000  0.999393         110000                10.1665 us\n",
      "         9     4       3  0.999318   0.998930 1.000000  0.999465         110000                 9.3692 us\n",
      "         9     5       3  0.999291   0.998887 1.000000  0.999443         110000                 9.3470 us\n",
      "        10     1       3  0.989673   0.984031 1.000000  0.991951         110000                 8.2113 us\n",
      "        10     2       3  0.999291   0.998887 1.000000  0.999443         110000                 7.1217 us\n",
      "        10     3       3  0.999300   0.998901 1.000000  0.999450         110000                 7.3901 us\n",
      "        10     4       3  0.999318   0.998930 1.000000  0.999465         110000                 7.0225 us\n",
      "        10     5       3  0.999282   0.998873 1.000000  0.999436         110000                 6.3955 us\n",
      "         4     1       4  0.890545   0.999586 0.828343  0.905943         110000                 6.4160 us\n",
      "         4     2       4  0.891609   0.999261 0.830286  0.906970         110000                 6.0194 us\n",
      "         4     3       4  0.891155   0.999294 0.829543  0.906541         110000                 6.8072 us\n",
      "         4     4       4  0.891591   0.999192 0.830314  0.906959         110000                 6.4855 us\n",
      "         4     5       4  0.892518   0.999468 0.831543  0.907805         110000                 5.4687 us\n",
      "         5     1       4  0.890400   0.999311 0.828343  0.905830         110000                 6.2132 us\n",
      "         5     2       4  0.885327   0.987528 0.830286  0.902106         110000                 6.4539 us\n",
      "         5     3       4  0.891209   0.999398 0.829543  0.906583         110000                 6.2445 us\n",
      "         5     4       4  0.891764   0.999518 0.830314  0.907093         110000                10.2683 us\n",
      "         5     5       4  0.892445   0.999330 0.831543  0.907748         110000                 9.2224 us\n",
      "         6     1       4  0.931418   0.996945 0.894971  0.943210         110000                 8.6936 us\n",
      "         6     2       4  0.890055   0.996331 0.830286  0.905762         110000                 9.4657 us\n",
      "         6     3       4  0.889191   0.995594 0.829543  0.905015         110000                 9.5576 us\n",
      "         6     4       4  0.932455   0.996367 0.897129  0.944147         110000                 9.4383 us\n",
      "         6     5       4  0.932291   0.996744 0.896529  0.943984         110000                 8.8085 us\n",
      "         7     1       4  0.930173   0.994776 0.894971  0.942238         110000                 9.7126 us\n",
      "         7     2       4  0.890118   0.996451 0.830286  0.905811         110000                 9.5501 us\n",
      "         7     3       4  0.931055   0.996168 0.895100  0.942934         110000                 9.2630 us\n",
      "         7     4       4  0.932391   0.996256 0.897129  0.944097         110000                 9.6184 us\n",
      "         7     5       4  0.932409   0.996950 0.896529  0.944076         110000                 8.8755 us\n",
      "         8     1       4  0.931282   0.996707 0.894971  0.943103         110000                 9.5942 us\n",
      "         8     2       4  0.926109   0.987673 0.895057  0.939087         110000                10.0728 us\n",
      "         8     3       4  0.931355   0.996691 0.895100  0.943168         110000                 8.9094 us\n",
      "         8     4       4  0.927691   0.988151 0.897129  0.940443         110000                 8.9412 us\n",
      "         8     5       4  0.890936   0.996491 0.831543  0.906575         110000                10.3156 us\n",
      "         9     1       4  0.931418   0.996945 0.894971  0.943210         110000                 9.4056 us\n",
      "         9     2       4  0.926682   0.988654 0.895057  0.939530         110000                 8.8805 us\n",
      "         9     3       4  0.890973   0.998951 0.829543  0.906399         110000                 8.9856 us\n",
      "         9     4       4  0.932836   0.997031 0.897129  0.944445         110000                10.2923 us\n",
      "         9     5       4  0.927282   0.988097 0.896529  0.940088         110000                 9.2043 us\n",
      "        10     1       4  0.931418   0.996945 0.894971  0.943210         110000                 8.9947 us\n",
      "        10     2       4  0.931427   0.996866 0.895057  0.943222         110000                 9.9836 us\n",
      "        10     3       4  0.931600   0.997120 0.895100  0.943360         110000                 9.4778 us\n",
      "        10     4       4  0.932609   0.996636 0.897129  0.944268         110000                 9.0011 us\n",
      "        10     5       4  0.932409   0.996950 0.896529  0.944076         110000                 9.7921 us\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_data = {}\n",
    "result_df, store_results_df = accumulate_results(result_sources, confusion_matrix_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee64a5-4404-4e51-aed9-cbcc52e1d352",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix (per client per Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369edecd-71ed-4395-9f49-15e6138f84fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "860421bd-e59e-4d79-be72-7bc5aa089873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    {\n",
    "        'client_id': 1,\n",
    "        'plot_name': 'Client 1',\n",
    "        'plot_position': [0, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 2,\n",
    "        'plot_name': 'Client 2',\n",
    "        'plot_position': [0, 1]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 3,\n",
    "        'plot_name': 'Client 3',\n",
    "        'plot_position': [1, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 4,\n",
    "        'plot_name': 'Client 4',\n",
    "        'plot_position': [1, 1]\n",
    "    }   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd872bc-f4dd-400d-9546-7751662492bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
